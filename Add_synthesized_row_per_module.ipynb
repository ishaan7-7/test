{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed53131-2219-4c86-a2f1-4784896191c0",
   "metadata": {},
   "source": [
    "### Add Scenario A rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fed3be-c277-49a2-aaaf-36ffabd0b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & config (Scenario A focused)\n",
    "import os, json, uuid, hashlib, math, logging, base64, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# Paths (keep your original)\n",
    "INPUT_CSV = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\engine_inference_decoded_json.csv\")\n",
    "OUT_CSV = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_engine_inference_scenarioA.csv\")\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Synthesis params (tweakable)\n",
    "RNG_SEED = 42\n",
    "RNG = np.random.RandomState(RNG_SEED)\n",
    "CHUNK_SIZE = 5000\n",
    "TOTAL_DAYS = 12   # Scenario A covers days 1..12\n",
    "# We'll generate exactly the hours defined in the original scenario A mapping (see Cell 4)\n",
    "USE_COPULA = False   # disabled for tuning stability (switch on later if needed)\n",
    "\n",
    "# Composite tuning (error-like: higher -> worse)\n",
    "WEIGHTS = {\n",
    "    \"recon_error_dense\": 0.6,\n",
    "    \"recon_error_lstm\": 0.25,\n",
    "    \"isolation_score\": 0.08,\n",
    "    \"kde_logp\": 0.04,\n",
    "    \"gmm_logp\": 0.03\n",
    "}\n",
    "COMPOSITE_GAIN = 6.0\n",
    "COMPOSITE_CLIP = (-6.0, 6.0)\n",
    "\n",
    "# Spike control (robust)\n",
    "SPIKE_PCTILE_LOW, SPIKE_PCTILE_HIGH = 1.0, 99.5\n",
    "MAX_SPIKE_FACTOR = 1.0   # max fraction of (p90 - p10) used as spike magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c46bb1-ea78-462e-8406-55f3ffc35114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utilities (hash, b64, empirical helpers, winsorize/log1p transforms)\n",
    "def sha256_hex_of_row(d: dict, exclude_keys=(\"row_hash\",)):\n",
    "    dd = {k:v for k,v in d.items() if k not in exclude_keys}\n",
    "    s = json.dumps(dd, sort_keys=True, ensure_ascii=False, separators=(',',':'))\n",
    "    return hashlib.sha256(s.encode('utf-8')).hexdigest()\n",
    "\n",
    "def to_base64_json(obj):\n",
    "    s = json.dumps(obj, separators=(',',':'))\n",
    "    return base64.b64encode(s.encode('utf-8')).decode('ascii')\n",
    "\n",
    "def format_timestamp_for_csv(ts: pd.Timestamp) -> str:\n",
    "    return ts.strftime(\"%Y-%m-%dT%H:%M:%S+0000\")\n",
    "\n",
    "def empirical_cdf_interpolator(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.size == 0:\n",
    "        return None, None\n",
    "    uvals, counts = np.unique(arr, return_counts=True)\n",
    "    cumprob = np.cumsum(counts) / counts.sum()\n",
    "    return uvals.astype(float), cumprob\n",
    "\n",
    "def invert_empirical(u, uvals, cumprob):\n",
    "    return float(np.interp(u, cumprob, uvals, left=uvals[0], right=uvals[-1]))\n",
    "\n",
    "def winsorize_series(arr, low_pct=1.0, high_pct=99.5):\n",
    "    lo = np.percentile(arr, low_pct)\n",
    "    hi = np.percentile(arr, high_pct)\n",
    "    return np.clip(arr, lo, hi), lo, hi\n",
    "\n",
    "def robust_scale_params(arr):\n",
    "    # returns median and scale (use p10-p90 or IQR fallback)\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.size == 0:\n",
    "        return 0.0, 1.0\n",
    "    median = float(np.median(arr))\n",
    "    p10, p90 = np.percentile(arr, [10,90])\n",
    "    scale = float(max(1e-6, p90 - p10))\n",
    "    return median, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7421c2-ab4a-4a9a-9a97-4f13a9a33c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === battery_inference_decoded_json.csv ===\n",
    "# ['row_hash', 'timestamp', 'date', 'source_id', 'kafka_key', 'offset', 'source_file', \n",
    "#  'battery_voltage_ecu_7ee', 'battery_current', 'battery_state_of_charge_soc_pct', 'battery_state_of_health_soh_pct', \n",
    "#  'battery_temperature_cell', 'hv_battery_pack_voltage', 'hv_battery_pack_current', 'internal_resistance_impedance', \n",
    "#  'alternator_load_pct', 'alternator_voltage_output', 'energy_consumption_per_km_wh_per_km_per_inr_per_km', \n",
    "#  'regenerative_energy_recovered', 'distance_to_empty_km', 'average_trip_speed_while_moving', 'average_trip_speed_overall',\n",
    "#  'ambient_air_temperature', 'barometer_android_device_mb', 'engine_coolant_temperature', 'charging_power_kw',\n",
    "#  'charging_efficiency_pct', 'auxiliary_12v_battery_current_draw', 'fuel_consumption_km_per_l_or_l_per_100_km', \n",
    "#  'co_emissions_instant_per_avg', 'alcohol_fuel_percentage', 'boost_commanded_per_measured', 'recon_error_dense',\n",
    "#  'recon_error_lstm', 'lstm_window_id', 'isolation_score', 'kde_logp', 'gmm_logp', 'composite_score', 'anomaly_label',\n",
    "#  'anomaly_severity', 'inference_run_id', 'inference_timestamp', 'processing_latency_ms', 'dense_per_feature_error_json', \n",
    "#  'explain_top_k_json', 'raw_model_outputs_json', 'dense_per_feature_error_json_decoded_json', 'explain_top_k_json_decoded_json', \n",
    "#  'raw_model_outputs_json_decoded_json']\n",
    "\n",
    "# === engine_inference_decoded_json.csv ===\n",
    "# ['row_hash', 'timestamp', 'date', 'source_id', 'kafka_key', 'offset', 'source_file', 'air_fuel_ratio_commanded_1', \n",
    "#  'air_fuel_ratio_measured_1', 'catalyst_temperature_bank_1_sensor_1', 'catalyst_temperature_bank_1_sensor_2', \n",
    "#  'engine_kw_at_the_wheels_kw', 'engine_load_absolute', 'engine_oil_temperature', 'engine_rpm_rpm', 'fuel_flow_rate_hour_l_hr', \n",
    "#  'fuel_trim_bank_1_long_term', 'fuel_trim_bank_1_short_term', 'mass_air_flow_rate_g_s', 'o2_sensor1_wide_range_current_ma',\n",
    "#  'o2_bank_1_sensor_2_voltage_v', 'timing_advance', 'turbo_boost_vacuum_gauge_psi', 'voltage_control_module_v', \n",
    "#  'volumetric_efficiency_calculated', 'ecu_7ea_engine_coolant_temperature', 'ecu_7ea_intake_air_temperature', \n",
    "#  'ecu_7eb_ambient_air_temp', 'ecu_7eb_engine_load', 'ecu_7eb_engine_rpm_rpm', 'ecu_7eb_speed_obd_km_h', \n",
    "#  'recon_error_dense', 'recon_error_lstm', 'lstm_window_id', 'isolation_score', 'kde_logp', 'gmm_logp', \n",
    "#  'composite_score', 'anomaly_label', 'anomaly_severity', 'inference_run_id', 'inference_timestamp', \n",
    "#  'processing_latency_ms', 'dense_per_feature_error_json', 'explain_top_k_json', 'raw_model_outputs_json', \n",
    "#  'dense_per_feature_error_json_decoded_json', 'explain_top_k_json_decoded_json', 'raw_model_outputs_json_decoded_json']\n",
    "\n",
    "# === body_inference_decoded_json.csv ===\n",
    "# ['row_hash', 'timestamp', 'date', 'source_id', 'kafka_key', 'offset', 'source_file', 'ambient_air_temperature_body', \n",
    "#  'cabin_temperature', 'cabin_humidity_pct', 'hvac_blower_speed', 'ac_compressor_load_pct', 'window_open_pct', \n",
    "#  'sunroof_position_pct', 'fuel_level_pct', 'distance_since_codes_cleared', 'distance_with_mil_lit', 'odometer_reading', \n",
    "#  'recon_error_dense', 'recon_error_lstm', 'lstm_window_id', 'isolation_score', 'kde_logp', 'gmm_logp', \n",
    "#  'composite_score', 'anomaly_label', 'anomaly_severity', 'inference_run_id', 'inference_timestamp', \n",
    "#  'processing_latency_ms', 'dense_per_feature_error_json', 'explain_top_k_json', 'raw_model_outputs_json', \n",
    "#  'dense_per_feature_error_json_decoded_json', 'explain_top_k_json_decoded_json', 'raw_model_outputs_json_decoded_json']\n",
    "\n",
    "# === tyre_inference_decoded_json.csv ===\n",
    "# ['row_hash', 'timestamp', 'date', 'source_id', 'kafka_key', 'offset', 'source_file', \n",
    "#  'bearing_vehicle_heading', 'longitudinal_acceleration', 'lateral_acceleration', 'yaw_rate', \n",
    "#  'stopping_distance', 'steering_angle_sensor', 'steering_torque_applied', 'suspension_height_per_deflection',\n",
    "#  'suspension_damper_force', 'vertical_acceleration', 'tyre_pressure_fl_psi', 'tyre_pressure_fr_psi', \n",
    "#  'tyre_pressure_rl_psi', 'tyre_pressure_rr_psi', 'tyre_temp_fl_c', 'tyre_temp_fr_c', 'tyre_temp_rl_c', 'tyre_temp_rr_c',\n",
    "#  'wheel_speed_fl_kmh', 'wheel_speed_fr_kmh', 'wheel_speed_rl_kmh', 'wheel_speed_rr_kmh', 'slip_ratio_fl', 'slip_ratio_fr', \n",
    "#  'slip_ratio_rl', 'slip_ratio_rr', 'tyre_wear_fl_pct', 'tyre_wear_fr_pct', 'tyre_wear_rl_pct', 'tyre_wear_rr_pct', 'tyre_load_fl_n', \n",
    "#  'tyre_load_fr_n', 'tyre_load_rl_n', 'tyre_load_rr_n', 'accel_x_g_tyre', 'accel_y_g_tyre', 'accel_z_g_tyre', \n",
    "#  'vibration_f1_hz', 'vibration_f2_hz', 'vibration_f3_hz', 'recon_error_dense', 'recon_error_lstm', 'lstm_window_id', \n",
    "#  'isolation_score', 'kde_logp', 'gmm_logp', 'composite_score', 'anomaly_label', 'anomaly_severity', \n",
    "#  'inference_run_id', 'inference_timestamp', 'processing_latency_ms', 'dense_per_feature_error_json', 'explain_top_k_json',\n",
    "#  'raw_model_outputs_json', 'dense_per_feature_error_json_decoded_json', 'explain_top_k_json_decoded_json', 'raw_model_outputs_json_decoded_json']\n",
    "\n",
    "# === transmission_inference_decoded_json.csv ===\n",
    "# ['row_hash', 'timestamp', 'date', 'source_id', 'kafka_key', 'offset', 'source_file',\n",
    "#  'engine_rpm', 'gear_position_actual', 'gear_commanded_target', 'transmission_oil_temperature',\n",
    "#  'transmission_oil_pressure', 'clutch_engagement_per_slip', 'torque_converter_slip_speed', \n",
    "#  'actual_engine_pct_torque', 'driver_demand_engine_pct_torque', 'engine_reference_torque_nm', \n",
    "#  'acceleration_sensor_total_g', 'throttle_position_manifold_pct', 'accelerator_pedal_position_d_per_e_per_f_pct',\n",
    "#  'air_fuel_ratio_commanded', 'air_fuel_ratio_measured', 'boost_pressure_commanded_a_per_b', \n",
    "#  'boost_pressure_sensor_a_per_b', 'engine_load_calculated_pct', 'engine_load_absolute_pct', \n",
    "#  'egr_commanded_pct', 'egr_error_pct', 'catalyst_temperatures_bank_sensors', 'bearing_heading_per_vehicle_yaw', \n",
    "#  'accel_x_g', 'accel_y_g', 'accel_z_g', 'vehicle_speed_kmh', 'recon_error_dense', 'recon_error_lstm', \n",
    "#  'lstm_window_id', 'isolation_score', 'kde_logp', 'gmm_logp', 'composite_score', 'anomaly_label', \n",
    "#  'anomaly_severity', 'inference_run_id', 'inference_timestamp', 'processing_latency_ms', \n",
    "#  'dense_per_feature_error_json', 'explain_top_k_json', 'raw_model_outputs_json', \n",
    "#  'dense_per_feature_error_json_decoded_json', 'explain_top_k_json_decoded_json', 'raw_model_outputs_json_decoded_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b6a84-44ce-4f31-be4b-efba52bc785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load empirical CSV & compute robust marginals/stats\n",
    "logging.info(\"Loading empirical CSV...\")\n",
    "df_emp = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "n_emp = len(df_emp)\n",
    "logging.info(f\"Empirical rows: {n_emp}, columns: {len(df_emp.columns)}\")\n",
    "\n",
    "# Candidate numeric columns (same list as your original; filtered by presence)\n",
    "NUMERIC_COLS_CANDIDATE = [\n",
    "    \"air_fuel_ratio_commanded_1\",\"air_fuel_ratio_measured_1\",\"catalyst_temperature_bank_1_sensor_1\",\"catalyst_temperature_bank_1_sensor_2\",\n",
    "    \"engine_kw_at_the_wheels_kw\",\"engine_load_absolute\",\n",
    "    \"engine_oil_temperature\",\"engine_rpm_rpm\",\"fuel_flow_rate_hour_l_hr\",\n",
    "    \"fuel_trim_bank_1_long_term\",\"fuel_trim_bank_1_short_term\",\"mass_air_flow_rate_g_s\",\n",
    "    \"o2_sensor1_wide_range_current_ma\",\n",
    "    \"o2_bank_1_sensor_2_voltage_v\",\"timing_advance\",\"turbo_boost_vacuum_gauge_psi\",\n",
    "    \"voltage_control_module_v\",\"volumetric_efficiency_calculated\",\n",
    "    \"ecu_7ea_engine_coolant_temperature\",\"ecu_7ea_intake_air_temperature\",\"ecu_7eb_ambient_air_temp\",\n",
    "    \"ecu_7eb_engine_load\",\"ecu_7eb_engine_rpm_rpm\",\"ecu_7eb_speed_obd_km_h\",\n",
    "    \"recon_error_dense\",\"recon_error_lstm\",\"isolation_score\",\"kde_logp\",\"gmm_logp\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUMERIC_COLS = [c for c in NUMERIC_COLS_CANDIDATE if c in df_emp.columns]\n",
    "logging.info(f\"Numeric cols present: {NUMERIC_COLS}\")\n",
    "\n",
    "# Precompute robust marginals: uvals/cumprob for each numeric column and robust stats\n",
    "empirical_marginals = {}\n",
    "for c in NUMERIC_COLS:\n",
    "    arr_raw = df_emp[c].dropna().astype(float).values\n",
    "    if arr_raw.size == 0:\n",
    "        empirical_marginals[c] = {\"uvals\": None, \"cumprob\": None, \"median\": 0.0, \"scale\":1.0, \"p1\":0.0, \"p995\":0.0}\n",
    "        continue\n",
    "    # winsorize to remove pathological extremes for building the empirical CDF used in invert_empirical\n",
    "    arr_win, p1, p995 = winsorize_series(arr_raw, SPIKE_PCTILE_LOW, SPIKE_PCTILE_HIGH)\n",
    "    uvals, cumprob = empirical_cdf_interpolator(arr_win)\n",
    "    median, scale = robust_scale_params(arr_win)\n",
    "    empirical_marginals[c] = {\"uvals\": uvals, \"cumprob\": cumprob, \"median\": median, \"scale\": scale, \"p1\": float(p1), \"p995\": float(p995)}\n",
    "logging.info(\"Prepared robust empirical marginals.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fe496-c4ea-4120-81cd-4a301ba84ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scenario A schedule (days 0..11) and helpers\n",
    "# We will use the same day-hours mapping you had for Scenario A\n",
    "day_hours = []\n",
    "for d in range(1,13):\n",
    "    if d <= 6:\n",
    "        day_hours.append(3)   # baseline\n",
    "    elif d <= 10:\n",
    "        day_hours.append(8)   # anomaly windows\n",
    "    elif d == 11:\n",
    "        day_hours.append(9)   # service day\n",
    "    else:\n",
    "        day_hours.append(4)   # post-service\n",
    "assert len(day_hours) == TOTAL_DAYS\n",
    "\n",
    "# choose_substate and modulation functions specialized for scenario A\n",
    "def choose_substate_for_time_within_day_scenarioA(second_of_day, H_seconds):\n",
    "    frac = second_of_day / max(1, H_seconds)\n",
    "    # map: first half baseline, middle pre_failure, then failure window near end of middle,\n",
    "    # service shortly after, post_service at very end.\n",
    "    if frac < 0.5:\n",
    "        return \"baseline\", frac/0.5\n",
    "    elif frac < 0.8:\n",
    "        # within pre_failure -> map to pre_failure or failure if near the final portion\n",
    "        rel = (frac - 0.5) / 0.3\n",
    "        # inject a failure in days where H_hours >= 6 (we'll handle in generator by checking day index)\n",
    "        return \"pre_failure\", rel\n",
    "    elif frac < 0.85:\n",
    "        return \"failure\", (frac-0.8)/0.05\n",
    "    elif frac < 0.90:\n",
    "        return \"service\", (frac-0.85)/0.05\n",
    "    else:\n",
    "        return \"post_service\", (frac-0.90)/0.10\n",
    "\n",
    "def modulation_M_scenarioA(substate, rel_pos, day_idx):\n",
    "    # substate -> scalar; pre_failure ramps more negative further into the Scenario A block\n",
    "    if substate == \"baseline\":\n",
    "        return RNG.normal(loc=0.0, scale=0.02)\n",
    "    if substate == \"pre_failure\":\n",
    "        # ramp from small negative to stronger negative depending on day_idx (later days worse)\n",
    "        base = -0.05 - 0.012 * max(0, (day_idx - 3))  # worsen after day 4 gradually\n",
    "        return base * (0.2 + rel_pos) + RNG.normal(0, 0.02)\n",
    "    if substate == \"failure\":\n",
    "        return float(RNG.uniform(-0.75, -0.55) + RNG.normal(0, 0.03))\n",
    "    if substate == \"service\":\n",
    "        return float(RNG.uniform(0.45, 0.7) + RNG.normal(0, 0.03))\n",
    "    if substate == \"post_service\":\n",
    "        return float(0.05 + RNG.normal(0, 0.02))\n",
    "    return float(RNG.normal(0, 0.02))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759cb99-c385-489e-958e-a8859c4a87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Row generator (Scenario A only) - marginal sampling + robust spike + composite (error-like)\n",
    "def pick_alpha_for_col(col_name):\n",
    "    col_lower = col_name.lower()\n",
    "    if \"recon_error\" in col_lower or \"dense\" in col_lower or \"lstm\" in col_lower:\n",
    "        return 0.9\n",
    "    if \"isolation\" in col_lower:\n",
    "        return 0.5\n",
    "    if \"kde\" in col_lower or \"gmm\" in col_lower or \"logp\" in col_lower:\n",
    "        return 0.35\n",
    "    if any(k in col_lower for k in [\"battery\",\"current\",\"voltage\",\"temperature\",\"resistance\",\"soc\",\"soh\"]):\n",
    "        return 0.08\n",
    "    return 0.03\n",
    "\n",
    "# robust transform for recon errors: log1p + winsorize by precomputed percentiles\n",
    "RECON_COLS = [c for c in [\"recon_error_dense\",\"recon_error_lstm\"] if c in empirical_marginals]\n",
    "\n",
    "def compute_composite_score_row(row_vals):\n",
    "    # Build normalized components robustly and ensure composite_score is error-like (higher = worse)\n",
    "    comps = {}\n",
    "    # dense\n",
    "    for key, weight_key in [(\"recon_error_dense\",\"recon_error_dense\"),\n",
    "                            (\"recon_error_lstm\",\"recon_error_lstm\"),\n",
    "                            (\"isolation_score\",\"isolation_score\"),\n",
    "                            (\"kde_logp\",\"kde_logp\"),\n",
    "                            (\"gmm_logp\",\"gmm_logp\")]:\n",
    "        val = float(row_vals.get(key, 0.0) or 0.0)\n",
    "        if key in RECON_COLS:\n",
    "            # log-transform for heavy tails\n",
    "            val_t = math.log1p(abs(val)) * (1.0 if val >=0 else -1.0)\n",
    "            median = 0.0\n",
    "            scale = 1.0\n",
    "            # compute median/scale from marginal precomputed log1p stats (approx)\n",
    "            meta = empirical_marginals.get(key)\n",
    "            if meta:\n",
    "                # approximate by median and scale of winsorized raw -> but we used log1p at inference;\n",
    "                # so use median of log1p(empirical) for centering (compute lazily if not present)\n",
    "                if \"log_median\" not in meta:\n",
    "                    arr = df_emp[key].dropna().astype(float).values\n",
    "                    arr_win, _, _ = winsorize_series(arr, SPIKE_PCTILE_LOW, SPIKE_PCTILE_HIGH)\n",
    "                    meta[\"log_median\"] = float(np.median(np.log1p(np.abs(arr_win)) * np.sign(arr_win)))\n",
    "                    p10, p90 = np.percentile(np.log1p(np.abs(arr_win)) * np.sign(arr_win), [10,90])\n",
    "                    meta[\"log_scale\"] = float(max(1e-6, p90 - p10))\n",
    "                median = meta[\"log_median\"]\n",
    "                scale = meta[\"log_scale\"]\n",
    "            normed = (val_t - median) / max(scale, 1e-6)\n",
    "        else:\n",
    "            meta = empirical_marginals.get(key)\n",
    "            median = meta[\"median\"] if meta else 0.0\n",
    "            scale = meta[\"scale\"] if meta else 1.0\n",
    "            normed = (val - median) / max(scale, 1e-6)\n",
    "        comps[key] = normed\n",
    "\n",
    "    # weighted raw\n",
    "    score_raw = 0.0\n",
    "    # Only include weights that exist in WEIGHTS\n",
    "    for k in [\"recon_error_dense\",\"recon_error_lstm\",\"isolation_score\",\"kde_logp\",\"gmm_logp\"]:\n",
    "        w = WEIGHTS.get(k, 0.0)\n",
    "        score_raw += w * comps.get(k, 0.0)\n",
    "\n",
    "    # clip raw and logistic\n",
    "    score_raw = float(np.clip(score_raw, COMPOSITE_CLIP[0], COMPOSITE_CLIP[1]))\n",
    "    s = 1.0 / (1.0 + math.exp(-COMPOSITE_GAIN * score_raw))\n",
    "    composite_score = float(np.clip(s, 0.0, 1.0))  # error-like: higher => worse\n",
    "    return composite_score\n",
    "\n",
    "def generate_scenarioA_rows(day_hours, start_date, rng=RNG):\n",
    "    offset_counter = 0\n",
    "    inference_ts_now = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f+00:00\")\n",
    "    for day_idx in range(len(day_hours)):\n",
    "        H_hours = day_hours[day_idx]\n",
    "        H_seconds = int(H_hours * 3600)\n",
    "        current_date = start_date + pd.Timedelta(days=day_idx)\n",
    "        day_start_time = pd.Timestamp(current_date) + pd.Timedelta(hours=8)\n",
    "\n",
    "        for sec in range(H_seconds):\n",
    "            ts = day_start_time + pd.Timedelta(seconds=sec)\n",
    "            timestamp_str = format_timestamp_for_csv(pd.Timestamp(ts).tz_localize(None))\n",
    "            date_str = timestamp_str[:10]\n",
    "\n",
    "            substate, rel_pos = choose_substate_for_time_within_day_scenarioA(sec, H_seconds)\n",
    "            M = modulation_M_scenarioA(substate, rel_pos, day_idx)\n",
    "\n",
    "            # sample marginals for each numeric column\n",
    "            row_vals = {}\n",
    "            for col in NUMERIC_COLS:\n",
    "                meta = empirical_marginals.get(col)\n",
    "                if meta is None or meta[\"uvals\"] is None:\n",
    "                    row_vals[col] = float(\"nan\")\n",
    "                    continue\n",
    "                # sample u and invert empirical\n",
    "                u = rng.uniform()\n",
    "                val = invert_empirical(u, meta[\"uvals\"], meta[\"cumprob\"])\n",
    "\n",
    "                # prepare alpha and spike bound\n",
    "                alpha = pick_alpha_for_col(col)\n",
    "                # robust spike magnitude: fraction of (p90 - p10) approximated by meta[\"scale\"]\n",
    "                spike_bound = MAX_SPIKE_FACTOR * meta[\"scale\"]\n",
    "                spike = 0.0\n",
    "                if substate == \"failure\":\n",
    "                    # directed spike that increases 'error' for recon errors (sign depends on nature)\n",
    "                    spike_loc = -1.0 * spike_bound  # negative direction in value space is not consistent across features,\n",
    "                                                     # but we treat this as additive magnitude for recon errors.\n",
    "                    spike = rng.normal(loc=spike_loc, scale=0.25 * spike_bound)\n",
    "\n",
    "                # apply modulation:\n",
    "                if \"recon_error\" in col.lower() or \"recon\" in col.lower() or \"dense\" in col.lower() or \"lstm\" in col.lower():\n",
    "                    # additive-dominant\n",
    "                    val_mod = val + alpha * (M * max(1.0, abs(val))) + spike + rng.normal(0, meta[\"scale\"] * 0.02)\n",
    "                else:\n",
    "                    # multiplicative small drift\n",
    "                    val_mod = val * (1.0 + alpha * M) + spike + rng.normal(0, meta[\"scale\"] * 0.02)\n",
    "\n",
    "                # winsorize/cap to empirical p1/p995\n",
    "                val_mod = float(np.clip(val_mod, meta[\"p1\"], meta[\"p995\"]))\n",
    "                row_vals[col] = val_mod\n",
    "\n",
    "            # compute composite score (error-like)\n",
    "            composite = compute_composite_score_row(row_vals)\n",
    "            # label and severity using error-like thresholds (higher error worse)\n",
    "            if composite > 0.6:\n",
    "                label, severity = \"anomaly\", 2\n",
    "            elif composite > 0.4:\n",
    "                label, severity = \"suspicious\", 1\n",
    "            else:\n",
    "                label, severity = \"normal\", 0\n",
    "\n",
    "            # explain_top_k (reuse your earlier logic but with robust dev calc)\n",
    "            def generate_explain_top_k_local(row_numeric_dict, rng_local=rng):\n",
    "                FEATURE_POOL = [c for c in NUMERIC_COLS if c not in (\"recon_error_dense\",\"recon_error_lstm\",\"isolation_score\",\"kde_logp\",\"gmm_logp\")]\n",
    "                deviations = []\n",
    "                for f in FEATURE_POOL:\n",
    "                    if f not in empirical_marginals:\n",
    "                        deviations.append((f, 0.0)); continue\n",
    "                    meta = empirical_marginals[f]\n",
    "                    med = meta[\"median\"]\n",
    "                    std = meta[\"scale\"]\n",
    "                    val = row_numeric_dict.get(f, 0.0)\n",
    "                    deviations.append((f, abs((val - med) / max(1e-6, std))))\n",
    "                deviations.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_candidates = deviations[:6]\n",
    "                feats = [t[0] for t in top_candidates]\n",
    "                weights = np.array([t[1] + 1e-6 for t in top_candidates], dtype=float)\n",
    "                probs = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights)/len(weights)\n",
    "                chosen = list(rng_local.choice(feats, size=min(3,len(feats)), replace=False, p=probs))\n",
    "                result = []\n",
    "                for f in chosen:\n",
    "                    dev = next((d for (ff,d) in deviations if ff==f), 0.0)\n",
    "                    contrib = float(np.clip(rng_local.uniform(0,1) * (0.3 + min(dev/3.0, 1.0)), 0.0, 1.0))\n",
    "                    result.append({\"feature\": f, \"contribution\": contrib})\n",
    "                decoded_json_str = json.dumps(result)\n",
    "                encoded = base64.b64encode(decoded_json_str.encode()).decode()\n",
    "                return decoded_json_str, encoded\n",
    "\n",
    "            decoded_explain, encoded_explain = generate_explain_top_k_local(row_vals)\n",
    "\n",
    "            # build final row dict\n",
    "            row = {}\n",
    "            row[\"row_hash\"] = None\n",
    "            row[\"timestamp\"] = timestamp_str\n",
    "            row[\"date\"] = date_str\n",
    "            row[\"source_id\"] = \"sim001\"\n",
    "            row[\"kafka_key\"] = \"sim001\"\n",
    "            row[\"offset\"] = offset_counter\n",
    "            row[\"source_file\"] = str(df_emp[\"source_file\"].iloc[0]) if \"source_file\" in df_emp.columns else \"C:\\\\source.csv\"\n",
    "\n",
    "            for c in NUMERIC_COLS:\n",
    "                row[c] = row_vals.get(c, float(\"nan\"))\n",
    "\n",
    "            row[\"composite_score\"] = composite\n",
    "            row[\"anomaly_label\"] = label\n",
    "            row[\"anomaly_severity\"] = severity\n",
    "            row[\"inference_run_id\"] = \"run-\" + uuid.uuid4().hex[:24]\n",
    "            row[\"inference_timestamp\"] = inference_ts_now\n",
    "            row[\"processing_latency_ms\"] = int(abs(rng.normal(loc=500, scale=200)))\n",
    "            row[\"explain_top_k_json_decoded_json\"] = decoded_explain\n",
    "            row[\"explain_top_k_json\"] = encoded_explain\n",
    "            row[\"lstm_window_id\"] = str(uuid.uuid4())\n",
    "            row[\"inference_window_id\"] = str(uuid.uuid4())\n",
    "\n",
    "            row[\"row_hash\"] = sha256_hex_of_row(row)\n",
    "            offset_counter += 1\n",
    "            yield row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689026f-d5cb-4d20-ba7c-9ffcf13512df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Streamed chunk write to CSV\n",
    "from pathlib import Path\n",
    "\n",
    "def write_synthetic_csv(out_path: Path, row_gen, chunk_size=CHUNK_SIZE, total_estimate=None):\n",
    "    first = True\n",
    "    written = 0\n",
    "    start_time = time.time()\n",
    "    with out_path.open(\"w\", newline='', encoding='utf-8') as fout:\n",
    "        chunk = []\n",
    "        for row in row_gen:\n",
    "            chunk.append(row)\n",
    "            if len(chunk) >= chunk_size:\n",
    "                df_chunk = pd.DataFrame(chunk)\n",
    "                if first:\n",
    "                    df_chunk.to_csv(fout, index=False)\n",
    "                    first = False\n",
    "                else:\n",
    "                    df_chunk.to_csv(fout, index=False, header=False)\n",
    "                written += len(chunk)\n",
    "                elapsed = time.time() - start_time\n",
    "                logging.info(f\"WROTE {written} rows (elapsed {elapsed:.1f}s)\")\n",
    "                chunk = []\n",
    "            if total_estimate and written >= total_estimate:\n",
    "                break\n",
    "        if chunk:\n",
    "            df_chunk = pd.DataFrame(chunk)\n",
    "            if first:\n",
    "                df_chunk.to_csv(fout, index=False)\n",
    "            else:\n",
    "                df_chunk.to_csv(fout, index=False, header=False)\n",
    "            written += len(chunk)\n",
    "            logging.info(f\"FINAL WRITE: total rows written {written}\")\n",
    "    return written\n",
    "\n",
    "# compute estimated target for scenario A (rows = sum hours * 3600)\n",
    "estimated_rows_A = sum(int(h * 3600) for h in day_hours)\n",
    "logging.info(f\"Estimated Scenario A rows: {estimated_rows_A}\")\n",
    "start_date = None\n",
    "# derive start_date from empirical CSV min timestamp if available\n",
    "if \"timestamp\" in df_emp.columns and df_emp['timestamp'].dropna().shape[0]>0:\n",
    "    try:\n",
    "        start_date = pd.to_datetime(df_emp['timestamp'].dropna().astype(str).iloc[0]).normalize()\n",
    "    except Exception:\n",
    "        start_date = pd.Timestamp.now().normalize()\n",
    "else:\n",
    "    start_date = pd.Timestamp.now().normalize()\n",
    "\n",
    "row_gen = generate_scenarioA_rows(day_hours, start_date, rng=RNG)\n",
    "logging.info(\"Starting Scenario A synthesis to CSV ...\")\n",
    "written = write_synthetic_csv(OUT_CSV, row_gen, chunk_size=CHUNK_SIZE, total_estimate=estimated_rows_A)\n",
    "logging.info(f\"Synthesis complete. Rows written: {written}. File: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66684346-8b2f-462d-97cc-af9d19b96013",
   "metadata": {},
   "source": [
    "### Add Final Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a19a6-ab82-41a2-a77b-2c1488d5e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Inspect existing CSV to find last timestamp, last offset, and row count\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.info(\"Inspecting existing synthetic CSV to compute continuation parameters...\")\n",
    "\n",
    "OUT_CSV = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_engine_inference_scenarioA.csv\")\n",
    "if not OUT_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Existing synthetic CSV not found: {OUT_CSV}\")\n",
    "\n",
    "# read in chunks just last chunk to be memory-friendly\n",
    "last_row = None\n",
    "last_offset = None\n",
    "last_ts = None\n",
    "rows_count = 0\n",
    "usecols = [\"timestamp\", \"offset\"] if \"offset\" in df_emp.columns else [\"timestamp\"]\n",
    "# robust chunk read -- use iterator to only keep final chunk\n",
    "for chunk in pd.read_csv(OUT_CSV, usecols=[\"timestamp\",\"offset\"], chunksize=200000, low_memory=False):\n",
    "    rows_count += len(chunk)\n",
    "    last_row = chunk.iloc[-1]\n",
    "\n",
    "if last_row is None:\n",
    "    raise RuntimeError(\"Could not locate any rows in existing CSV (unexpected).\")\n",
    "\n",
    "last_ts = pd.to_datetime(str(last_row[\"timestamp\"]))\n",
    "last_offset = int(last_row[\"offset\"]) if \"offset\" in last_row.index or \"offset\" in last_row else rows_count - 1\n",
    "\n",
    "logging.info(f\"Existing CSV rows: {rows_count}, last timestamp: {last_ts}, last offset: {last_offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445646b-f6b4-4b5d-bfe2-909593628b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Backup the existing CSV before appending\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "backup_ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "backup_path = OUT_CSV.with_name(OUT_CSV.stem + f\"_backup_{backup_ts}\" + OUT_CSV.suffix)\n",
    "shutil.copy2(OUT_CSV, backup_path)\n",
    "logging.info(f\"Backup created: {backup_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d679f5-f135-4fce-9a28-023fd09df4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generator wrapper to produce exactly N additional rows, cycling the Scenario A day_hours pattern\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "def generate_scenarioA_rows_n(target_rows: int, base_day_hours: Iterable[int], start_date, rng=None, initial_offset=0):\n",
    "    \"\"\"\n",
    "    Produce exactly `target_rows` rows, cycling the base_day_hours sequence as needed.\n",
    "    Uses generate_scenarioA_rows logic for per-day generation but will stop when target reached.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = RNG  # default RNG from notebook\n",
    "    # create an infinite day_hours iterator that cycles through the baseline pattern\n",
    "    cyc = itertools.cycle(base_day_hours)\n",
    "    written = 0\n",
    "    day_idx = 0\n",
    "    # We'll call the original row generator per-day but limit rows overall\n",
    "    while written < target_rows:\n",
    "        H_hours = next(cyc)  # using cycle ensures pattern repeats\n",
    "        # We'll construct a per-day single-day day_hours list to call existing generator for one day\n",
    "        per_day_gen = generate_scenarioA_rows([H_hours], start_date + pd.Timedelta(days=day_idx), rng=rng)\n",
    "        for row in per_day_gen:\n",
    "            # fix offset to be continuous from existing file\n",
    "            row[\"offset\"] = initial_offset + written + 1  # +1 so next offset is after last_offset\n",
    "            yield row\n",
    "            written += 1\n",
    "            if written >= target_rows:\n",
    "                break\n",
    "        day_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec6ff8-781e-40ee-9d04-0494df347b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Append new block (28 days / 529200 rows) to existing CSV\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "TARGET_ROWS_APPEND = 529200  # exact number you specified for 28 days\n",
    "# Determine next start_date as day after last row's date (normalize to midnight)\n",
    "start_date_newblock = pd.to_datetime(last_ts).normalize() + pd.Timedelta(days=1)\n",
    "\n",
    "# new RNG seed so appended block is different yet reproducible\n",
    "NEW_RNG_SEED = RNG_SEED + 1\n",
    "new_rng = np.random.RandomState(NEW_RNG_SEED)\n",
    "\n",
    "# build generator for N rows using original base day_hours (the 12-day mapping), but cycled\n",
    "row_gen_n = generate_scenarioA_rows_n(\n",
    "    target_rows=TARGET_ROWS_APPEND,\n",
    "    base_day_hours=day_hours,   # original pattern from Cell 4\n",
    "    start_date=start_date_newblock,\n",
    "    rng=new_rng,\n",
    "    initial_offset=last_offset\n",
    ")\n",
    "\n",
    "# append to CSV in chunks (mirror write_synthetic_csv but append mode and no header)\n",
    "chunk_size = CHUNK_SIZE\n",
    "written_append = 0\n",
    "start_time = time.time()\n",
    "first_chunk = True\n",
    "with OUT_CSV.open(\"a\", newline='', encoding='utf-8') as fout:\n",
    "    chunk = []\n",
    "    for row in row_gen_n:\n",
    "        chunk.append(row)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            df_chunk = pd.DataFrame(chunk)\n",
    "            # write without header to append\n",
    "            df_chunk.to_csv(fout, index=False, header=False)\n",
    "            written_append += len(chunk)\n",
    "            elapsed = time.time() - start_time\n",
    "            logging.info(f\"APPENDED {written_append} rows (elapsed {elapsed:.1f}s)\")\n",
    "            chunk = []\n",
    "    # final remainder\n",
    "    if chunk:\n",
    "        df_chunk = pd.DataFrame(chunk)\n",
    "        df_chunk.to_csv(fout, index=False, header=False)\n",
    "        written_append += len(chunk)\n",
    "        logging.info(f\"FINAL APPEND: appended rows {written_append}\")\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "logging.info(f\"Append complete. Rows appended: {written_append}. Time: {elapsed_total:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3df5c4-cdb1-4b35-a17e-d50d51a3a691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
