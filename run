# File: C:\streaming_emulator\writer_service\src\stream_processor.py
import time
import logging
from pyspark.sql.functions import from_json, col, current_timestamp
from infrastructure import setup_environment

# Initialize Environment BEFORE importing Spark
setup_environment()

from spark_factory import get_spark_session
from schema_loader import load_all_schemas

# Logging
logging.basicConfig(
    level=logging.INFO, 
    format='[%(asctime)s] %(levelname)s: %(message)s'
)
logger = logging.getLogger("StreamProcessor")

def run_writer_pipeline():
    # 1. Start Spark
    spark = get_spark_session("Phase2_Writer")
    spark.sparkContext.setLogLevel("WARN")

    # 2. Load Schemas
    try:
        schemas = load_all_schemas()
    except Exception as e:
        logger.error(f"Failed to load schemas: {e}")
        return

    active_streams = []

    # 3. Dynamic Stream Generation
    for module_name, schema in schemas.items():
        logger.info(f"ðŸš€ Initializing Stream for Module: {module_name}")
        
        # A. Define Inputs
        topic_pattern = f"sim.*_{module_name}"
        checkpoint_dir = "C:/streaming_emulator/data/checkpoints/writer"
        checkpoint_path = f"{checkpoint_dir}/{module_name}"
        output_path = f"C:/streaming_emulator/data/delta/bronze/{module_name}"
        
        # B. Read Stream (Kafka)
        # Using parentheses allows safe multi-line formatting
        raw_stream = (
            spark.readStream
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribePattern", topic_pattern)
            .option("startingOffsets", "earliest")
            .option("failOnDataLoss", "false")
            .load()
        )

        # C. Transform & Flatten
        # We perform column selection step-by-step to keep lines short
        parsed_df = raw_stream.select(
            from_json(col("value").cast("string"), schema).alias("parsed")
        )
        
        flattened_df = parsed_df.select(
            col("parsed.metadata.row_hash").alias("row_hash"),
            col("parsed.metadata.ingest_ts").alias("ingest_ts"),
            current_timestamp().alias("writer_ts"),
            col("parsed.metadata.vehicle_id").alias("source_id"),
            col("parsed.data.*")
        )

        # D. Deduplicate (Exactly-Once)
        deduped_stream = flattened_df.dropDuplicates(["row_hash"])

        # E. Write Stream (Delta Lake)
        query = (
            deduped_stream.writeStream
            .format("delta")
            .outputMode("append")
            .partitionBy("source_id")
            .option("checkpointLocation", checkpoint_path)
            .option("mergeSchema", "true")
            .start(output_path)
        )
        
        active_streams.append(query)
        logger.info(f"âœ… Stream started: {module_name}")

    # 4. Keep Alive
    logger.info(f"ðŸ”¥ {len(active_streams)} streams active. Waiting for data...")
    try:
        spark.streams.awaitAnyTermination()
    except KeyboardInterrupt:
        logger.info("ðŸ›‘ Stopping Writer Service...")
        for q in active_streams:
            q.stop()

if __name__ == "__main__":
    run_writer_pipeline()
