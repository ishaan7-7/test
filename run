# File: C:\streaming_emulator\writer_service\src\schema_loader.py
import json
import logging
from pathlib import Path
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, 
    LongType, BooleanType, TimestampType
)

# Setup logging
logger = logging.getLogger("SchemaLoader")
logger.setLevel(logging.INFO)

def get_project_root():
    """Finds the root C:/streaming_emulator directory."""
    current = Path(__file__).parent.absolute()
    for parent in [current] + list(current.parents):
        if parent.name == "streaming_emulator":
            return parent
    return Path(r"C:\streaming_emulator")

def _map_dtype_to_spark(dtype_str):
    """
    Maps Pandas/JSON strings to Spark SQL Types.
    """
    dtype_str = str(dtype_str).lower()
    
    if 'float' in dtype_str:
        return DoubleType()
    elif 'int' in dtype_str:
        return LongType()
    elif 'bool' in dtype_str:
        return BooleanType()
    elif 'datetime' in dtype_str:
        return TimestampType()
    else:
        return StringType()

def load_all_schemas():
    """
    Reads contracts/master.json and returns a dictionary of Spark Schemas.
    
    Structure Returned:
    {
        "engine": StructType([...]),
        "battery": StructType([...]),
        ...
    }
    """
    root = get_project_root()
    master_path = root / "contracts" / "master.json"
    
    if not master_path.exists():
        logger.error(f"❌ Contract missing at: {master_path}")
        raise FileNotFoundError(f"master.json not found at {master_path}")

    try:
        with open(master_path, 'r') as f:
            contract = json.load(f)
            
        schemas = {}
        
        # We iterate over every module defined in master.json (engine, battery, etc.)
        for module_name, fields in contract.items():
            
            # 1. Build the 'data' struct (Dynamic from contract)
            data_fields = []
            for col_name, dtype in fields.items():
                spark_type = _map_dtype_to_spark(dtype)
                data_fields.append(StructField(col_name, spark_type, True))
            
            # 2. Build the 'metadata' struct (Fixed for Protocol)
            # This captures the Phase 1 Ingest Timestamp for Latency calculations
            metadata_fields = [
                StructField("row_hash", StringType(), False),  # Critical for Idempotency
                StructField("ingest_ts", StringType(), True),  # Sent as ISO String from Phase 1
                StructField("source_id", StringType(), True),
                StructField("trace_id", StringType(), True)
            ]
            
            # 3. Construct the Composite Schema expected from Kafka
            # Payload = { "metadata": {...}, "data": {...} }
            composite_schema = StructType([
                StructField("metadata", StructType(metadata_fields), True),
                StructField("data", StructType(data_fields), True)
            ])
            
            schemas[module_name] = composite_schema
            logger.debug(f"✅ Loaded schema for: {module_name}")
            
        logger.info(f"✅ Successfully loaded {len(schemas)} schemas from master.json")
        return schemas

    except Exception as e:
        logger.error(f"❌ Failed to parse master.json: {e}")
        raise

if __name__ == "__main__":
    # Quick test when running directly
    logging.basicConfig()
    s = load_all_schemas()
    print("Test Load Complete.")
