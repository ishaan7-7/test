# File: C:\streaming_emulator\writer_service\src\spark_factory.py
import sys
from pyspark.sql import SparkSession
from pathlib import Path
import logging

logger = logging.getLogger("SparkFactory")

def get_spark_session(app_name="WriterService"):
    """
    Creates a Production-Ready Spark Session with:
    1. Delta Lake support.
    2. Kafka support (FIXED).
    3. Local Warehouse (avoids C:/tmp errors).
    4. Correct Windows/Python 3.13 stability configs.
    """
    warehouse_dir = Path("C:/streaming_emulator/data/spark-warehouse")
    warehouse_dir.mkdir(parents=True, exist_ok=True)
    
    # Define Checkpoint Root for reliability
    checkpoint_root = Path("C:/streaming_emulator/data/checkpoints")
    checkpoint_root.mkdir(parents=True, exist_ok=True)

    logger.info(f"âš¡ Starting Spark Session: {app_name}")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .master("local[4]") \
        .config("spark.driver.host", "127.0.0.1") \
        .config("spark.driver.bindAddress", "127.0.0.1") \
        .config("spark.sql.warehouse.dir", str(warehouse_dir.as_uri())) \
        # CRITICAL FIX: Download the Kafka Connector from Maven
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,io.delta:delta-spark_2.12:3.2.1") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.python.use.daemon", "false") \
        .config("spark.python.worker.reuse", "false") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.databricks.delta.schema.autoMerge.enabled", "true") \
        .getOrCreate()
        
    return spark
