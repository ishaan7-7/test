# File: C:\streaming_emulator\writer_service\src\spark_factory.py
import sys
from pyspark.sql import SparkSession
from pathlib import Path
import logging

logger = logging.getLogger("SparkFactory")

def get_spark_session(app_name="WriterService"):
    """
    Creates a Production-Ready Spark Session with:
    1. Delta Lake support.
    2. Kafka support (Auto-downloaded).
    3. Local Warehouse (avoids C:/tmp errors).
    4. Correct Windows/Python 3.13 stability configs.
    """
    warehouse_dir = Path("C:/streaming_emulator/data/spark-warehouse")
    warehouse_dir.mkdir(parents=True, exist_ok=True)
    
    # Define Checkpoint Root for reliability
    checkpoint_root = Path("C:/streaming_emulator/data/checkpoints")
    checkpoint_root.mkdir(parents=True, exist_ok=True)

    logger.info(f"âš¡ Starting Spark Session: {app_name}")
    
    # Note: We use parentheses () here instead of backslashes \
    # This is safer for copy-pasting and allows comments between lines if needed.
    spark = (SparkSession.builder 
        .appName(app_name) 
        .master("local[4]") 
        .config("spark.driver.host", "127.0.0.1") 
        .config("spark.driver.bindAddress", "127.0.0.1") 
        .config("spark.sql.warehouse.dir", str(warehouse_dir.as_uri())) 
        # Kafka & Delta JARs
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,io.delta:delta-spark_2.12:3.2.1") 
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") 
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") 
        .config("spark.python.use.daemon", "false") 
        .config("spark.python.worker.reuse", "false") 
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") 
        .config("spark.databricks.delta.schema.autoMerge.enabled", "true") 
        .getOrCreate()
    )
        
    return spark
