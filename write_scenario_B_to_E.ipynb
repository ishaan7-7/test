{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fea0d-40d8-4476-9e32-5bad29dc6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: summary of CSV row counts and date ranges (first 12 days, then 7-day blocks from day 13)\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "# Load only timestamp column (fast)\n",
    "df_ts = pd.read_csv(CSV_PATH, usecols=[\"timestamp\"], low_memory=False)\n",
    "n_total = len(df_ts)\n",
    "print(f\"Total rows in CSV: {n_total:,}\")\n",
    "\n",
    "# Parse timestamps (UTC aware)\n",
    "df_ts[\"timestamp\"] = pd.to_datetime(df_ts[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "if df_ts[\"timestamp\"].isna().any():\n",
    "    n_bad = int(df_ts[\"timestamp\"].isna().sum())\n",
    "    print(f\"Warning: {n_bad} rows had unparsable timestamps and will be ignored for date calculations.\")\n",
    "\n",
    "# overall start / end (based on non-null timestamps)\n",
    "valid = df_ts[\"timestamp\"].dropna()\n",
    "if valid.empty:\n",
    "    raise ValueError(\"No valid timestamps found in CSV.\")\n",
    "start_ts = valid.min()\n",
    "end_ts = valid.max()\n",
    "print(f\"Overall start timestamp: {start_ts} (UTC)\")\n",
    "print(f\"Overall end   timestamp: {end_ts} (UTC)\")\n",
    "\n",
    "# Normalize start date (midnight) and build day index for each row\n",
    "start_date = start_ts.normalize()  # Timestamp at 00:00 of first day\n",
    "df_ts[\"date_only\"] = df_ts[\"timestamp\"].dt.date\n",
    "df_ts[\"day_index\"] = ((df_ts[\"timestamp\"].dt.normalize() - start_date).dt.days).astype(\"Int64\")\n",
    "\n",
    "# Rows in first 12 days (day_index 0..11)\n",
    "mask_first12 = (df_ts[\"day_index\"].notna()) & (df_ts[\"day_index\"] >= 0) & (df_ts[\"day_index\"] <= 11)\n",
    "rows_first12 = int(mask_first12.sum())\n",
    "print(f\"\\nRows in first 12 days (day 0..11 starting {start_date.date()}): {rows_first12:,}\")\n",
    "\n",
    "# Now compute counts for contiguous 7-day blocks starting from day_index 12 (i.e., day 13)\n",
    "# Determine max day index present\n",
    "max_day_index = int(df_ts[\"day_index\"].dropna().max())\n",
    "if max_day_index < 12:\n",
    "    print(\"\\nNo rows beyond the first 12 days to form 7-day blocks.\")\n",
    "else:\n",
    "    blocks = []\n",
    "    block_start = 12\n",
    "    block_id = 0\n",
    "    while block_start <= max_day_index:\n",
    "        block_end = block_start + 6  # inclusive\n",
    "        mask_block = (df_ts[\"day_index\"].notna()) & (df_ts[\"day_index\"] >= block_start) & (df_ts[\"day_index\"] <= block_end)\n",
    "        cnt = int(mask_block.sum())\n",
    "        start_day = (start_date + pd.Timedelta(days=block_start)).date()\n",
    "        end_day = (start_date + pd.Timedelta(days=min(block_end, max_day_index))).date()\n",
    "        blocks.append((block_id+1, block_start, min(block_end, max_day_index), start_day, end_day, cnt))\n",
    "        block_id += 1\n",
    "        block_start += 7\n",
    "\n",
    "    print(\"\\n7-day blocks starting from day 13 (day_index 12):\")\n",
    "    for b in blocks:\n",
    "        bid, dstart, dend, sdate, edate, cnt = b\n",
    "        print(f\" Block {bid}: day_index {dstart}..{dend}  ({sdate} -> {edate})  rows: {cnt:,}\")\n",
    "\n",
    "# Also print total rows covered by first12 + all blocks (should be <= total)\n",
    "covered = rows_first12 + sum(b[-1] for b in blocks) if 'blocks' in locals() else rows_first12\n",
    "print(f\"\\nRows covered by reported windows: {covered:,} (of {n_total:,})\")\n",
    "if covered != n_total:\n",
    "    print(\"Note: remaining rows may have unparsable timestamps or fall outside counted day range.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91af260-96b2-4bb9-8ccb-82f7a2dc41a0",
   "metadata": {},
   "source": [
    "### Scenario B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c4f3d-f1c3-42da-b677-8b0ce7e5aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports, configuration, backup\n",
    "import os, shutil, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "CSV_PATH = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found at {CSV_PATH}. Update CSV_PATH if needed.\")\n",
    "\n",
    "# Row (1-based) where Scenario B begins\n",
    "ROW_B_1BASED = 226_801\n",
    "\n",
    "# RNG seed (reproducible)\n",
    "SEED = 20251122\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# Backup path (one-shot)\n",
    "BACKUP_PATH = CSV_PATH.with_name(CSV_PATH.stem + \"_backup_before_scenarioB.csv\")\n",
    "if not BACKUP_PATH.exists():\n",
    "    shutil.copy2(CSV_PATH, BACKUP_PATH)\n",
    "    print(\"Backup created:\", BACKUP_PATH)\n",
    "else:\n",
    "    print(\"Backup already present:\", BACKUP_PATH)\n",
    "\n",
    "print(\"Config ready. Seed:\", SEED, \"Scenario-B start row (1-based):\", ROW_B_1BASED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf72cc3-4e3e-43dc-bd53-8268b78f5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: load CSV and determine the Scenario-B timestamp window\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "n_rows = len(df)\n",
    "print(\"CSV rows:\", n_rows)\n",
    "\n",
    "if ROW_B_1BASED < 1 or ROW_B_1BASED > n_rows:\n",
    "    raise IndexError(f\"Provided start row {ROW_B_1BASED} is out of range (1..{n_rows})\")\n",
    "\n",
    "# 1-based -> 0-based index\n",
    "start_idx0 = ROW_B_1BASED - 1\n",
    "row0 = df.iloc[start_idx0]\n",
    "if \"timestamp\" not in df.columns:\n",
    "    raise KeyError(\"'timestamp' column missing in CSV\")\n",
    "\n",
    "# parse timestamps (we will add a parsed column, used later)\n",
    "df[\"timestamp_parsed\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "\n",
    "t0 = df.loc[start_idx0, \"timestamp_parsed\"]\n",
    "t_end = t0 + pd.Timedelta(days=7)\n",
    "\n",
    "print(\"Scenario B window timestamps:\")\n",
    "print(\"  start (t0) =\", t0)\n",
    "print(\"  end (t0+7d)=\", t_end)\n",
    "\n",
    "# mask of rows to modify (timestamps in [t0, t0+7d) )\n",
    "mask_B = (df[\"timestamp_parsed\"] >= t0) & (df[\"timestamp_parsed\"] < t_end)\n",
    "n_mask = mask_B.sum()\n",
    "if n_mask == 0:\n",
    "    raise ValueError(\"No rows found in the 7-day window starting at row {} timestamp {}\".format(ROW_B_1BASED, t0))\n",
    "print(\"Rows in Scenario-B window:\", n_mask, \" (row indices approx\", start_idx0, \"to\", start_idx0 + n_mask - 1, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff4da0-c02e-484e-a788-7a523db2182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Cell 3: build a smooth 7-day mean curve for Scenario B (timezone-safe, vectorized)\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# rows_idx, mask_B, df, t0, t_end computed in Cell 2 already\n",
    "rows_idx = np.flatnonzero(mask_B.values)\n",
    "series_B_ts = df.loc[mask_B, \"timestamp_parsed\"]  # this is a pandas Series of tz-aware Timestamps\n",
    "\n",
    "# Vectorized seconds offset relative to t0 (tz-aware safe)\n",
    "secs_offset = (series_B_ts - t0).dt.total_seconds().astype(int).values\n",
    "if secs_offset.size == 0:\n",
    "    raise ValueError(\"No seconds offsets computed for Scenario B rows\")\n",
    "\n",
    "# window length in seconds (cover full continuous 7-day window)\n",
    "window_sec = int((t_end - t0).total_seconds())\n",
    "print(\"Scenario-B continuous window length (seconds):\", window_sec)\n",
    "\n",
    "# Define fractional allocation across the 7 days (global, tuneable)\n",
    "frac_map = {\n",
    "    \"baseline_long\": 0.35,        # day 1-2 mostly healthy\n",
    "    \"pre_agitation\": 0.20,        # small agitation (day 3)\n",
    "    \"event_rise\": 0.15,           # ramp up into event (end of day 3)\n",
    "    \"failure_peak\": 0.10,         # main transient (short)\n",
    "    \"service_drop\": 0.05,         # quick service\n",
    "    \"recovery\": 0.15              # rest of days 5-7\n",
    "}\n",
    "# normalize and compute integer boundaries\n",
    "total_f = sum(frac_map.values())\n",
    "cursor = 0\n",
    "segments = {}\n",
    "for k,v in frac_map.items():\n",
    "    L = int(round((v/total_f) * window_sec))\n",
    "    segments[k] = (cursor, max(cursor + L - 1, cursor))\n",
    "    cursor += L\n",
    "# fix last segment to end exactly at window_sec-1\n",
    "segments[list(segments.keys())[-1]] = (segments[list(segments.keys())[-1]][0], window_sec - 1)\n",
    "\n",
    "print(\"Segments (sec ranges):\")\n",
    "for k,(a,b) in segments.items():\n",
    "    print(f\"  {k}: {a} -> {b} (len {b-a+1})\")\n",
    "\n",
    "# bands for composite_score (error-like)\n",
    "B = {\n",
    "    \"baseline_low\": 0.15, \"baseline_high\": 0.25,\n",
    "    \"pre_low\": 0.25, \"pre_high\": 0.45,\n",
    "    \"rise_low\": 0.45, \"rise_high\": 0.75,\n",
    "    \"failure_low\": 0.75, \"failure_high\": 0.99,\n",
    "    \"service_low\": 0.08, \"service_high\": 0.20,\n",
    "    \"recovery_low\": 0.12, \"recovery_high\": 0.25\n",
    "}\n",
    "\n",
    "def linear(a,b,t): return a + (b-a)*t\n",
    "def triangular_peak(low, high, rel):\n",
    "    tri = 1.0 - abs(2.0*rel - 1.0)\n",
    "    return low + (high - low) * tri\n",
    "\n",
    "# create smooth mean trend across all seconds\n",
    "trend = np.zeros(window_sec, dtype=float)\n",
    "for s in range(window_sec):\n",
    "    # find segment\n",
    "    for name, (a,b) in segments.items():\n",
    "        if a <= s <= b:\n",
    "            rel = (s - a) / max(1, (b - a))\n",
    "            seg = name\n",
    "            break\n",
    "    else:\n",
    "        seg = \"baseline_long\"; rel = 0.0\n",
    "    if seg == \"baseline_long\":\n",
    "        trend[s] = linear(B[\"baseline_low\"], B[\"baseline_high\"], rel)\n",
    "    elif seg == \"pre_agitation\":\n",
    "        trend[s] = linear(B[\"pre_low\"], B[\"pre_high\"], rel)\n",
    "    elif seg == \"event_rise\":\n",
    "        trend[s] = linear(B[\"rise_low\"], B[\"rise_high\"], rel**1.1)\n",
    "    elif seg == \"failure_peak\":\n",
    "        trend[s] = triangular_peak(B[\"failure_low\"], B[\"failure_high\"], rel)\n",
    "    elif seg == \"service_drop\":\n",
    "        # drop from failure mid to service band\n",
    "        failure_mid = (B[\"failure_low\"] + B[\"failure_high\"]) / 2.0\n",
    "        target = linear(B[\"service_low\"], B[\"service_high\"], rng.rand())\n",
    "        trend[s] = linear(failure_mid, target, rel)\n",
    "    elif seg == \"recovery\":\n",
    "        # recovery ramp to baseline\n",
    "        start_val = linear(B[\"service_low\"], B[\"service_high\"], 0.5)\n",
    "        end_val = linear(B[\"recovery_low\"], B[\"recovery_high\"], 0.5)\n",
    "        trend[s] = linear(start_val, end_val, rel)\n",
    "    else:\n",
    "        trend[s] = linear(B[\"baseline_low\"], B[\"baseline_high\"], rel)\n",
    "\n",
    "# slight smoothing so the mean isn't piecewise\n",
    "trend_smooth = gaussian_filter1d(trend, sigma=30)  # smoothing across seconds\n",
    "trend_smooth = np.clip(trend_smooth, 0.0, 1.0)\n",
    "\n",
    "print(\"Trend created for Scenario B. Sample: \", trend_smooth[:5], \"...\", trend_smooth[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3205afe-1dcd-4d20-9814-284732223022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: generate realistic noise to add on top of the trend for the Scenario-B window\n",
    "# Low-frequency noise (LF): gaussian low-pass filtered white noise\n",
    "lf_sigma_seconds = 1800   # ~30 minutes; increase to make longer wiggles\n",
    "lf_white = rng.normal(0.0, 1.0, size=window_sec)\n",
    "lf_noise = gaussian_filter1d(lf_white, sigma=lf_sigma_seconds)\n",
    "# scale lf_noise to target amplitude\n",
    "lf_amp = 0.045\n",
    "lf_noise = lf_noise / (np.std(lf_noise) + 1e-12) * lf_amp\n",
    "\n",
    "# High-frequency heteroskedastic noise (HF): white noise whose amplitude scales with trend\n",
    "hf_base = 0.018\n",
    "hf_white = rng.normal(0.0, 1.0, size=window_sec)\n",
    "hf_noise = hf_white * hf_base * (0.6 + 0.8 * trend_smooth)  # more jitter during higher trend\n",
    "\n",
    "# Short correlated bursts\n",
    "burst_prob = 0.0012   # per second chance\n",
    "burst_amp = 0.10\n",
    "for t in range(window_sec):\n",
    "    if rng.rand() < burst_prob:\n",
    "        width = rng.randint(20, 300)  # 20s - 5min\n",
    "        start = t\n",
    "        end = min(window_sec - 1, t + width)\n",
    "        # triangular burst added\n",
    "        for j in range(start, end+1):\n",
    "            rel = (j - start) / max(1, (end - start))\n",
    "            burst_val = (1.0 - abs(2*rel - 1.0)) * burst_amp * (0.5 + rng.rand()*0.8)\n",
    "            lf_noise[j] += burst_val  # add into LF channel for coherence\n",
    "\n",
    "# Rare extreme glitch spikes (very small count)\n",
    "num_extreme = max(1, int(window_sec * 0.0002))  # tiny fraction\n",
    "extreme_positions = rng.choice(window_sec, size=num_extreme, replace=False)\n",
    "for p in extreme_positions:\n",
    "    lf_noise[p] += rng.uniform(0.15, 0.30)\n",
    "\n",
    "# Combine noises\n",
    "combined_noise = lf_noise + hf_noise\n",
    "# normalize to zero mean (avoid shifting trend mean)\n",
    "combined_noise = combined_noise - np.mean(combined_noise)\n",
    "print(\"Noise generated. lf_sigma_seconds:\", lf_sigma_seconds, \"lf_amp:\", lf_amp, \"hf_base:\", hf_base)\n",
    "print(\"Noise std:\", np.std(combined_noise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c882da5-8481-4cca-8e1a-3fd583275a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: compose final composite_score for all rows in the B window and assign back to df\n",
    "# Map timestamp -> seconds offset relative to t0 and fetch final value from trend_smooth + combined_noise\n",
    "\n",
    "# final_series (by second index)\n",
    "noise_gain = 1.0  # tuneable\n",
    "final_series = np.clip(trend_smooth + noise_gain * combined_noise, 0.0, 1.0)\n",
    "\n",
    "# map unique timestamps in masked rows to seconds (cache)\n",
    "unique_ts = df.loc[mask_B, \"timestamp_parsed\"].unique()\n",
    "ts_to_sec = {ts: int((pd.Timestamp(ts) - t0).total_seconds()) for ts in unique_ts}\n",
    "\n",
    "# apply per-row small jitter and occasional micro-spike\n",
    "row_indices = df.index[mask_B]\n",
    "new_cs = df.loc[mask_B, \"composite_score\"].values.copy()  # placeholder\n",
    "i = 0\n",
    "for idx, ts in zip(row_indices, df.loc[mask_B, \"timestamp_parsed\"]):\n",
    "    sec = ts_to_sec[ts]\n",
    "    sec = max(0, min(window_sec - 1, sec))\n",
    "    value = float(final_series[sec])\n",
    "    # occasional tiny micro-spike outside main failure (rare)\n",
    "    if rng.rand() < 0.001:\n",
    "        value = min(0.99, value + rng.uniform(0.08, 0.25))\n",
    "    # small row-level jitter\n",
    "    value += rng.normal(0.0, 0.005)\n",
    "    value = float(np.clip(value, 0.0, 1.0))\n",
    "    new_cs[i] = value\n",
    "    i += 1\n",
    "\n",
    "# write back only masked rows\n",
    "df.loc[mask_B, \"composite_score\"] = new_cs\n",
    "# recompute composite_health and labels for masked rows\n",
    "df.loc[mask_B, \"composite_health\"] = 1.0 - df.loc[mask_B, \"composite_score\"]\n",
    "\n",
    "def assign_label(val):\n",
    "    if val > 0.6:\n",
    "        return \"anomaly\", 2\n",
    "    elif val > 0.4:\n",
    "        return \"suspicious\", 1\n",
    "    else:\n",
    "        return \"normal\", 0\n",
    "\n",
    "labels = [assign_label(v) for v in df.loc[mask_B, \"composite_score\"].values]\n",
    "lab, sev = zip(*labels)\n",
    "df.loc[mask_B, \"anomaly_label\"] = lab\n",
    "df.loc[mask_B, \"anomaly_severity\"] = sev\n",
    "\n",
    "print(\"Assigned composite_score for mask (rows):\", n_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d0680-b51f-4985-8e61-a47ad4f79c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: atomic write back to CSV (overwrite existing file)\n",
    "tmp = CSV_PATH.with_suffix(\".tmp\")\n",
    "# drop helper parsed column before writing\n",
    "df_to_write = df.drop(columns=[\"timestamp_parsed\"])\n",
    "df_to_write.to_csv(tmp, index=False)\n",
    "os.replace(tmp, CSV_PATH)\n",
    "print(\"CSV updated in place at:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515df787-c072-4012-b338-884a9349f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: diagnostics for Scenario B window\n",
    "df_check = pd.read_csv(CSV_PATH, usecols=[\"timestamp\",\"composite_score\"])\n",
    "df_check[\"timestamp\"] = pd.to_datetime(df_check[\"timestamp\"], utc=True)\n",
    "# filter to B window\n",
    "mask_check = (df_check[\"timestamp\"] >= t0) & (df_check[\"timestamp\"] < t_end)\n",
    "df_b = df_check.loc[mask_check].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "print(\"Rows in B-diagnostics:\", len(df_b))\n",
    "\n",
    "# per-day summary over the 7-day window\n",
    "df_b[\"date_only\"] = df_b[\"timestamp\"].dt.date\n",
    "perday = df_b.groupby(\"date_only\")[\"composite_score\"].agg(['count','mean','std','min','max']).reset_index()\n",
    "print(\"\\nPer-day composite_score summary for Scenario B window:\")\n",
    "print(perday.to_string(index=False))\n",
    "\n",
    "# detect anomaly segments (composite_score > 0.6)\n",
    "threshold = 0.6\n",
    "df_b[\"is_anom\"] = (df_b[\"composite_score\"] > threshold).astype(int)\n",
    "df_b[\"seg_change\"] = df_b[\"is_anom\"].diff().fillna(df_b[\"is_anom\"])\n",
    "df_b[\"seg_id\"] = (df_b[\"is_anom\"] != df_b[\"is_anom\"].shift(1)).cumsum()\n",
    "segments = []\n",
    "for seg_id, grp in df_b.groupby(\"seg_id\"):\n",
    "    if grp[\"is_anom\"].iloc[0] == 1:\n",
    "        start = grp[\"timestamp\"].iloc[0]\n",
    "        end = grp[\"timestamp\"].iloc[-1]\n",
    "        dur_s = (end - start).total_seconds()\n",
    "        segments.append({\"seg_id\": int(seg_id), \"start\": str(start), \"end\": str(end), \"duration_s\": int(dur_s), \"n_points\": int(len(grp))})\n",
    "segments_sorted = sorted(segments, key=lambda x: x[\"duration_s\"], reverse=True)\n",
    "print(f\"\\nDetected {len(segments)} anomaly segments in B-window (threshold > {threshold}). Top segments:\")\n",
    "for s in segments_sorted[:10]:\n",
    "    print(f\"  seg {s['seg_id']}: {s['start']} -> {s['end']}, dur_s={s['duration_s']}, n={s['n_points']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d0d69-3d3a-4fe1-bbda-d3ee4d85be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Compressed timeline plot for first 12 days (Scenario A) + next 7 days (Scenario B),\n",
    "# with B-window main-peak marker.\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "# --- Load timestamps + composite_score only (fast) ---\n",
    "use_cols = [\"timestamp\", \"composite_score\"]\n",
    "df = pd.read_csv(CSV_PATH, usecols=use_cols, low_memory=False)\n",
    "df = df.dropna(subset=[\"timestamp\", \"composite_score\"]).reset_index(drop=True)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# --- Identify Scenario A first-12-days window start (min timestamp normalized) ---\n",
    "start_ts = df[\"timestamp\"].min()\n",
    "start_date = start_ts.normalize()  # midnight UTC of first day\n",
    "first12_end = start_date + pd.Timedelta(days=12)  # exclusive\n",
    "# Attempt to locate the Scenario-B start (prefer timestamp at row 226,801 if present)\n",
    "ROW_B_1BASED = 226_801\n",
    "t0 = None\n",
    "if len(df) >= ROW_B_1BASED:\n",
    "    try:\n",
    "        t0_candidate = pd.to_datetime(df.loc[ROW_B_1BASED - 1, \"timestamp\"], utc=True)\n",
    "        # ensure candidate is after first12_end; otherwise fallback\n",
    "        if t0_candidate >= first12_end:\n",
    "            t0 = t0_candidate\n",
    "    except Exception:\n",
    "        t0 = None\n",
    "\n",
    "# Fallback: if t0 couldn't be taken from the row, assume B starts exactly at day 13 (first12_end)\n",
    "if t0 is None:\n",
    "    t0 = first12_end\n",
    "scenarioB_start = t0\n",
    "scenarioB_end = scenarioB_start + pd.Timedelta(days=7)\n",
    "\n",
    "print(f\"Plot window: {start_date.date()} (start)  ->  {scenarioB_end.date()} (end)\")\n",
    "print(\"Using Scenario-B start (t0) =\", scenarioB_start)\n",
    "\n",
    "# --- Filter rows to the combined window: first 12 days + next 7 days (19 days total from start_date) ---\n",
    "combined_start = start_date\n",
    "combined_end = scenarioB_end  # exclusive end\n",
    "mask = (df[\"timestamp\"] >= combined_start) & (df[\"timestamp\"] < combined_end)\n",
    "df_sel = df.loc[mask].copy().reset_index(drop=True)\n",
    "if df_sel.empty:\n",
    "    raise ValueError(\"No rows found in the requested combined window. Check timestamps/row indices.\")\n",
    "\n",
    "print(f\"Rows selected for plotting: {len(df_sel):,} (from {df_sel['timestamp'].min()} to {df_sel['timestamp'].max()})\")\n",
    "\n",
    "# --- Compress timeline: remove idle gaps larger than GAP_MINUTES ---\n",
    "GAP_MINUTES = 30\n",
    "gap_thresh = pd.Timedelta(minutes=GAP_MINUTES)\n",
    "\n",
    "ts = df_sel[\"timestamp\"]\n",
    "diffs = ts.diff().fillna(pd.Timedelta(seconds=0))\n",
    "is_segment_start = (diffs > gap_thresh)\n",
    "segment_id = is_segment_start.cumsum()\n",
    "\n",
    "# compute cumulative shifts for each segment (vectorized-ish)\n",
    "prev_end = ts.iloc[0]\n",
    "shifts = pd.Series(pd.Timedelta(0), index=df_sel.index)\n",
    "cumulative_shift = pd.Timedelta(0)\n",
    "for seg_idx, (_, grp) in enumerate(df_sel.groupby(segment_id)):\n",
    "    seg_start = grp[\"timestamp\"].iloc[0]\n",
    "    seg_end = grp[\"timestamp\"].iloc[-1]\n",
    "    if seg_idx == 0:\n",
    "        cumulative_shift = pd.Timedelta(0)\n",
    "    else:\n",
    "        gap = seg_start - prev_end\n",
    "        remove = gap\n",
    "        cumulative_shift += remove\n",
    "    shifts.loc[grp.index] = cumulative_shift\n",
    "    prev_end = seg_end\n",
    "\n",
    "df_sel[\"shifted_ts\"] = df_sel[\"timestamp\"] - shifts.values\n",
    "\n",
    "# --- Resample for 1-minute resolution then compute 1-hour rolling mean (on shifted index) ---\n",
    "df_shift = df_sel.set_index(\"shifted_ts\").sort_index()\n",
    "resampled = df_shift[\"composite_score\"].resample(\"3T\").mean().interpolate(limit_direction='both')\n",
    "hr_line = resampled.rolling(window=60, min_periods=1).mean()\n",
    "\n",
    "# --- Find main peak inside B-window (by original timestamps) to mark on plot ---\n",
    "mask_b = (df_sel[\"timestamp\"] >= scenarioB_start) & (df_sel[\"timestamp\"] < scenarioB_end)\n",
    "if mask_b.any():\n",
    "    df_b = df_sel.loc[mask_b]\n",
    "    peak_idx = df_b[\"composite_score\"].idxmax()\n",
    "    peak_row = df_sel.loc[peak_idx]\n",
    "    peak_shifted_ts = peak_row[\"shifted_ts\"]\n",
    "    peak_val = float(peak_row[\"composite_score\"])\n",
    "    peak_ts = peak_row[\"timestamp\"]\n",
    "    print(f\"Main B-window peak at {peak_ts} (shifted {peak_shifted_ts}) value={peak_val:.4f}\")\n",
    "else:\n",
    "    peak_shifted_ts = None\n",
    "    print(\"No rows inside the B-window within the selected combined window (unexpected).\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.scatter(df_shift.index, df_shift[\"composite_score\"], s=6, alpha=0.20, color=\"tab:blue\", label=\"composite_score (points)\")\n",
    "plt.plot(hr_line.index, hr_line.values, linewidth=2.0, color=\"#D62728\", label=\"1-hour rolling mean\")\n",
    "\n",
    "# Shade/annotate the B-window on the shifted axis if available\n",
    "if mask_b.any():\n",
    "    b_masked = df_sel.loc[mask_b]\n",
    "    b_start_shifted = b_masked[\"shifted_ts\"].iloc[0]\n",
    "    b_end_shifted = b_masked[\"shifted_ts\"].iloc[-1]\n",
    "    plt.axvspan(b_start_shifted, b_end_shifted, color=\"gray\", alpha=0.12, label=\"Scenario B window\")\n",
    "\n",
    "# mark peak\n",
    "if peak_shifted_ts is not None:\n",
    "    plt.axvline(peak_shifted_ts, color=\"black\", linestyle=\"--\", linewidth=1.2, label=\"B-window peak\")\n",
    "    plt.plot([peak_shifted_ts], [peak_val], marker=\"o\", color=\"black\", markersize=6)\n",
    "    plt.annotate(f\"Peak {peak_val:.2f}\\n{peak_ts.strftime('%Y-%m-%d %H:%M')}\", \n",
    "                 xy=(peak_shifted_ts, peak_val), xytext=(8, 8), textcoords=\"offset points\", fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.title(\"Composite Score — First 12 days (Scenario A) + next 7 days (Scenario B)\\nCompressed operating timeline (gaps >30min removed)\")\n",
    "plt.xlabel(\"Compressed operating time (datetime-like, original gaps removed)\")\n",
    "plt.ylabel(\"Composite Score (error-like)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12319e8-8132-4edb-b6fe-4e5faecf9c40",
   "metadata": {},
   "source": [
    "### Scenario C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6bb99-5093-4241-9968-0f2b6efb15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df['timestamp_parsed'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "total_rows = len(df)\n",
    "\n",
    "print(\"Total rows:\", total_rows)\n",
    "\n",
    "# SCENARIO A/B/C INDEX RANGES\n",
    "A_end = 226_800\n",
    "B_end = 320_400\n",
    "C_start_idx = 320_400          # 0-based\n",
    "C_end_idx   = 475_200 - 1      # 0-based inclusive end\n",
    "\n",
    "print(\"\\nScenario A rows: 1 → 226800\")\n",
    "print(\"Scenario B rows: 226801 → 320400\")\n",
    "print(\"Scenario C rows: 320401 → 475200\\n\")\n",
    "\n",
    "print(\"Using 0-based indices:\")\n",
    "print(\"Scenario C index start:\", C_start_idx)\n",
    "print(\"Scenario C index end  :\", C_end_idx)\n",
    "\n",
    "# Derive timestamps for C-window\n",
    "C_start_ts = df.loc[C_start_idx, 'timestamp_parsed']\n",
    "C_end_ts   = C_start_ts + pd.Timedelta(days=7)\n",
    "\n",
    "print(\"\\nScenario C time window:\")\n",
    "print(\"C_start_ts:\", C_start_ts)\n",
    "print(\"C_end_ts  :\", C_end_ts)\n",
    "\n",
    "# Mask for timestamps inside window\n",
    "mask_C = (df['timestamp_parsed'] >= C_start_ts) & (df['timestamp_parsed'] < C_end_ts)\n",
    "\n",
    "print(\"\\nRows inside the timestamp window:\", mask_C.sum())\n",
    "print(\"Rows inside index window        :\", (df.index >= C_start_idx) & (df.index <= C_end_idx).sum())\n",
    "\n",
    "# Combined strict mask: must be within BOTH index + timestamp range\n",
    "mask_C_strict = mask_C & (df.index.to_series().between(C_start_idx, C_end_idx))\n",
    "print(\"Final strict Scenario-C row count:\", mask_C_strict.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ffa98-726e-4e61-a4ba-4c389b1a407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Build time axis for full 7-day C-window\n",
    "C_window_secs = int((C_end_ts - C_start_ts).total_seconds())\n",
    "print(\"Scenario C window seconds:\", C_window_secs)\n",
    "\n",
    "# Baseline near-healthy, small upward drift\n",
    "base_start = 0.16\n",
    "base_end   = 0.22\n",
    "\n",
    "trend = np.linspace(base_start, base_end, C_window_secs)\n",
    "\n",
    "# slight smoothing for realism\n",
    "trend = gaussian_filter1d(trend, sigma=1800)\n",
    "\n",
    "# clamp\n",
    "trend = np.clip(trend, 0.0, 1.0)\n",
    "\n",
    "print(\"Trend sample:\", trend[:5], \"...\", trend[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46442f5-7dce-40ef-a5e3-f459eb77a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "BURSTS_PER_DAY = 12               # tuneable\n",
    "NUM_BURSTS = BURSTS_PER_DAY * 7\n",
    "\n",
    "burst_starts = rng.integers(0, C_window_secs, size=NUM_BURSTS)\n",
    "burst_dur    = rng.integers(20, 120, size=NUM_BURSTS)   # 20–120 seconds\n",
    "burst_amp    = rng.uniform(0.25, 0.55, size=NUM_BURSTS) # add on top of trend\n",
    "\n",
    "bursts = []\n",
    "for s, d, a in zip(burst_starts, burst_dur, burst_amp):\n",
    "    e = min(s + d, C_window_secs - 1)\n",
    "    bursts.append((s, e, a))\n",
    "\n",
    "print(\"Generated bursts:\", len(bursts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e904f4-756e-4165-b719-f9251363f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-frequency noise\n",
    "lf = gaussian_filter1d(rng.normal(0, 1, C_window_secs), sigma=3600)\n",
    "lf = lf / lf.std() * 0.04   # amplitude\n",
    "\n",
    "# High-frequency noise\n",
    "hf = rng.normal(0, 0.02, C_window_secs)\n",
    "\n",
    "noise = lf + hf\n",
    "noise = noise - noise.mean()\n",
    "\n",
    "# Add bursts\n",
    "noise_burst = noise.copy()\n",
    "for s, e, amp in bursts:\n",
    "    dur = e - s + 1\n",
    "    rel = np.linspace(0,1,dur)\n",
    "    envelope = 1 - np.abs(2*rel - 1)   # triangular\n",
    "    noise_burst[s:e+1] += envelope * amp\n",
    "\n",
    "# Final clamp\n",
    "final_series = np.clip(trend + noise_burst, 0.0, 1.0)\n",
    "\n",
    "print(\"Final series sample:\", final_series[:5], \"...\", final_series[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32567c28-d472-439c-85cf-4b8a5d7af3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute seconds offset for each strict Scenario-C row\n",
    "offset_secs = ((df.loc[mask_C_strict, 'timestamp_parsed'] - C_start_ts)\n",
    "                 .dt.total_seconds()\n",
    "                 .astype(int))\n",
    "\n",
    "values_C = final_series[offset_secs.values]\n",
    "\n",
    "# Apply small per-row jitter\n",
    "values_C = np.clip(values_C + rng.normal(0,0.005,len(values_C)), 0, 1)\n",
    "\n",
    "# Update df\n",
    "df.loc[mask_C_strict, 'composite_score'] = values_C\n",
    "df.loc[mask_C_strict, 'composite_health'] = 1 - values_C\n",
    "\n",
    "print(\"Updated Scenario-C rows:\", mask_C_strict.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f961b-e16a-4300-bc8e-9270fc1e35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tmp = CSV_PATH.with_suffix(\".tmp\")\n",
    "df.to_csv(tmp, index=False)\n",
    "os.replace(tmp, CSV_PATH)\n",
    "\n",
    "print(\"CSV updated successfully:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e85c97-aec9-43fa-8257-d458fcfebbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Cell 15: diagnostics and compressed timeline plot for A (first 12 days) + B + C\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "CSV_PATH = r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\"\n",
    "\n",
    "# Re-load timestamps+composite for plotting in a safe way\n",
    "use_cols = [\"timestamp\", \"composite_score\"]\n",
    "df_plot = pd.read_csv(CSV_PATH, usecols=use_cols)\n",
    "df_plot = df_plot.dropna(subset=[\"timestamp\", \"composite_score\"]).reset_index(drop=True)\n",
    "df_plot[\"timestamp\"] = pd.to_datetime(df_plot[\"timestamp\"], utc=True)\n",
    "df_plot = df_plot.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Determine start_date (first day) and B and C windows using known indices and timestamps\n",
    "start_ts = df_plot[\"timestamp\"].min()\n",
    "start_date = start_ts.normalize()\n",
    "first12_end = start_date + pd.Timedelta(days=12)\n",
    "\n",
    "# Attempt to reuse previously computed B_start/B_end/C_start/C_end if present in notebook memory,\n",
    "# else compute from index-based canonical boundaries we agreed on\n",
    "try:\n",
    "    B_start = t0   # earlier scenario B start (if available)\n",
    "    B_end   = t_end\n",
    "except NameError:\n",
    "    # derive B_start as first timestamp after first12_end\n",
    "    B_start = df_plot[df_plot[\"timestamp\"] >= first12_end][\"timestamp\"].min()\n",
    "    B_end   = B_start + pd.Timedelta(days=7)\n",
    "\n",
    "# canonical C index window: rows 320401..475200 (1-based)\n",
    "# map that to timestamps defensively\n",
    "# (we will compute C_start based on row index 320401 -> 0-based 320400)\n",
    "total_rows = len(df_plot)\n",
    "idx_c_start_0 = 320400\n",
    "idx_c_end_0 = 475200 - 1\n",
    "\n",
    "if idx_c_start_0 < total_rows:\n",
    "    C_start = df_plot.loc[idx_c_start_0, \"timestamp\"]\n",
    "else:\n",
    "    raise IndexError(f\"CSV has only {total_rows} rows but expected index {idx_c_start_0} for C start.\")\n",
    "\n",
    "C_end = C_start + pd.Timedelta(days=7)\n",
    "\n",
    "print(\"Plot windows:\\n A: {} -> {}\\n B: {} -> {}\\n C: {} -> {}\".format(start_date, first12_end, B_start, B_end, C_start, C_end))\n",
    "\n",
    "# Select combined window = first12_start .. C_end\n",
    "combined_start = start_date\n",
    "combined_end = C_end\n",
    "mask_combined = (df_plot[\"timestamp\"] >= combined_start) & (df_plot[\"timestamp\"] < combined_end)\n",
    "df_sel = df_plot.loc[mask_combined].copy().reset_index(drop=True)\n",
    "if df_sel.empty:\n",
    "    raise ValueError(\"No rows found in the combined plotting window. Check indices/timestamps.\")\n",
    "\n",
    "print(\"Rows in combined plotting window:\", len(df_sel))\n",
    "\n",
    "# compress timeline by removing gaps > 30 minutes\n",
    "GAP_MINUTES = 30\n",
    "gap_thresh = pd.Timedelta(minutes=GAP_MINUTES)\n",
    "ts = df_sel[\"timestamp\"]\n",
    "diffs = ts.diff().fillna(pd.Timedelta(seconds=0))\n",
    "is_segment_start = (diffs > gap_thresh)\n",
    "segment_id = is_segment_start.cumsum()\n",
    "\n",
    "prev_end = ts.iloc[0]\n",
    "shifts = pd.Series(pd.Timedelta(0), index=df_sel.index)\n",
    "cumulative_shift = pd.Timedelta(0)\n",
    "for seg_idx, (_, grp) in enumerate(df_sel.groupby(segment_id)):\n",
    "    seg_start = grp[\"timestamp\"].iloc[0]\n",
    "    seg_end = grp[\"timestamp\"].iloc[-1]\n",
    "    if seg_idx == 0:\n",
    "        cumulative_shift = pd.Timedelta(0)\n",
    "    else:\n",
    "        gap = seg_start - prev_end\n",
    "        remove = gap\n",
    "        cumulative_shift += remove\n",
    "    shifts.loc[grp.index] = cumulative_shift\n",
    "    prev_end = seg_end\n",
    "\n",
    "df_sel[\"shifted_ts\"] = df_sel[\"timestamp\"] - shifts.values\n",
    "\n",
    "# compute 1-minute resample + 1-hour rolling mean\n",
    "df_shift = df_sel.set_index(\"shifted_ts\").sort_index()\n",
    "resampled = df_shift[\"composite_score\"].resample(\"1T\").mean().interpolate(limit_direction='both')\n",
    "hr_line = resampled.rolling(window=60, min_periods=1).mean()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.scatter(df_shift.index, df_shift[\"composite_score\"], s=6, alpha=0.18, color=\"tab:blue\", label=\"composite_score (points)\")\n",
    "plt.plot(hr_line.index, hr_line.values, linewidth=2.0, color=\"#D62728\", label=\"1-hour rolling mean\")\n",
    "\n",
    "# compute shifted boundaries for B and C windows using timestamp-based selection (robust)\n",
    "def shifted_span_for_window(window_start, window_end, df_sel):\n",
    "    rows = df_sel[(df_sel[\"timestamp\"] >= window_start) & (df_sel[\"timestamp\"] < window_end)]\n",
    "    if rows.empty:\n",
    "        return None, None\n",
    "    return rows[\"shifted_ts\"].iloc[0], rows[\"shifted_ts\"].iloc[-1]\n",
    "\n",
    "b_start_shifted, b_end_shifted = shifted_span_for_window(B_start, B_end, df_sel)\n",
    "c_start_shifted, c_end_shifted = shifted_span_for_window(C_start, C_end, df_sel)\n",
    "\n",
    "if b_start_shifted is not None:\n",
    "    plt.axvspan(b_start_shifted, b_end_shifted, color=\"gray\", alpha=0.12, label=\"Scenario B window\")\n",
    "if c_start_shifted is not None:\n",
    "    plt.axvspan(c_start_shifted, c_end_shifted, color=\"orange\", alpha=0.12, label=\"Scenario C window\")\n",
    "\n",
    "# annotate top 3 peaks in C-window (if any)\n",
    "if c_start_shifted is not None:\n",
    "    rows_c = df_sel[(df_sel[\"timestamp\"] >= C_start) & (df_sel[\"timestamp\"] < C_end)]\n",
    "    if not rows_c.empty:\n",
    "        top3 = rows_c.nlargest(3, \"composite_score\")\n",
    "        for _, r in top3.iterrows():\n",
    "            shifted = r[\"shifted_ts\"]\n",
    "            val = r[\"composite_score\"]\n",
    "            plt.plot(shifted, val, marker=\"o\", color=\"black\")\n",
    "            plt.annotate(f\"{val:.2f}\\n{r['timestamp'].strftime('%Y-%m-%d %H:%M')}\",\n",
    "                         xy=(shifted, val), xytext=(6,6), textcoords=\"offset points\", fontsize=8,\n",
    "                         bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.title(\"Composite Score — A (first 12 days) + B (7d) + C (7d) — compressed timeline\")\n",
    "plt.xlabel(\"Compressed operating time (gaps >30min removed)\")\n",
    "plt.ylabel(\"Composite Score\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print diagnostics for C-window\n",
    "df_c = df_sel[(df_sel[\"timestamp\"] >= C_start) & (df_sel[\"timestamp\"] < C_end)].copy()\n",
    "print(\"\\nC-window per-day composite summary (if any):\")\n",
    "if not df_c.empty:\n",
    "    df_c[\"date_only\"] = df_c[\"timestamp\"].dt.date\n",
    "    perday = df_c.groupby(\"date_only\")[\"composite_score\"].agg(['count','mean','std','min','max']).reset_index()\n",
    "    print(perday.to_string(index=False))\n",
    "else:\n",
    "    print(\"No rows in C-window for diagnostics.\")\n",
    "\n",
    "# detect anomaly segments (threshold 0.6)\n",
    "if not df_c.empty:\n",
    "    thr = 0.6\n",
    "    df_c[\"is_anom\"] = (df_c[\"composite_score\"] > thr).astype(int)\n",
    "    df_c[\"seg_id\"] = (df_c[\"is_anom\"] != df_c[\"is_anom\"].shift(1)).cumsum()\n",
    "    segs = []\n",
    "    for seg_id, grp in df_c.groupby(\"seg_id\"):\n",
    "        if grp[\"is_anom\"].iloc[0] == 1:\n",
    "            start = grp[\"timestamp\"].iloc[0]; end = grp[\"timestamp\"].iloc[-1]\n",
    "            segs.append({\"seg_id\":int(seg_id), \"start\":str(start), \"end\":str(end), \"dur_s\":int((end-start).total_seconds()), \"n\":len(grp)})\n",
    "    segs_sorted = sorted(segs, key=lambda x: x[\"dur_s\"], reverse=True)\n",
    "    print(f\"\\nDetected {len(segs_sorted)} anomaly segments in C-window (threshold>{thr}). Top 10:\")\n",
    "    for s in segs_sorted[:10]:\n",
    "        print(f\"  seg {s['seg_id']}: start={s['start']} end={s['end']} dur_s={s['dur_s']} n={s['n']}\")\n",
    "else:\n",
    "    print(\"No anomaly segments in C-window.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46c080-fb9d-4eae-aecb-cf9b02fa472a",
   "metadata": {},
   "source": [
    "### Scenario D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec96559-174a-4c53-8241-b4d12f5fe166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Verify Scenario D canonical index window and derive timestamps (strict index enforcement)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "# Load minimal columns for speed\n",
    "df_meta = pd.read_csv(CSV_PATH, usecols=[\"timestamp\"], low_memory=False)\n",
    "df_meta[\"timestamp_parsed\"] = pd.to_datetime(df_meta[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "total_rows = len(df_meta)\n",
    "print(\"Total rows in CSV:\", total_rows)\n",
    "\n",
    "# Canonical 1-based ranges (as agreed)\n",
    "A_end_1 = 226_800\n",
    "B_end_1 = 320_400\n",
    "D_start_1 = 475_201\n",
    "D_end_1   = 604_800\n",
    "\n",
    "# Convert to 0-based indices\n",
    "D_start_idx0 = D_start_1 - 1\n",
    "D_end_idx0   = D_end_1 - 1\n",
    "\n",
    "if D_end_idx0 >= total_rows:\n",
    "    raise IndexError(f\"CSV has {total_rows} rows but expected D_end index {D_end_idx0} to exist.\")\n",
    "\n",
    "# Derive D window timestamps from strict index start\n",
    "D_start_ts = df_meta.loc[D_start_idx0, \"timestamp_parsed\"]\n",
    "D_end_ts_by_index = df_meta.loc[D_end_idx0, \"timestamp_parsed\"]\n",
    "D_end_ts = D_start_ts + pd.Timedelta(days=7)  # canonical 7-day window end (exclusive)\n",
    "\n",
    "print(\"Scenario D index range (1-based):\", D_start_1, \"->\", D_end_1)\n",
    "print(\"Scenario D index range (0-based):\", D_start_idx0, \"->\", D_end_idx0)\n",
    "print(\"Derived D_start_ts:\", D_start_ts)\n",
    "print(\"Derived D_end_ts (7 days after start):\", D_end_ts)\n",
    "print(\"Timestamp at D_end index (sanity):\", D_end_ts_by_index)\n",
    "\n",
    "# Build masks\n",
    "# timestamp-based\n",
    "df_meta[\"in_D_time\"] = (df_meta[\"timestamp_parsed\"] >= D_start_ts) & (df_meta[\"timestamp_parsed\"] < D_end_ts)\n",
    "# index-based\n",
    "df_meta[\"in_D_index\"] = df_meta.index.to_series().between(D_start_idx0, D_end_idx0)\n",
    "# strict intersection (use this for updates)\n",
    "df_meta[\"in_D_strict\"] = df_meta[\"in_D_time\"] & df_meta[\"in_D_index\"]\n",
    "\n",
    "n_time = int(df_meta[\"in_D_time\"].sum())\n",
    "n_index = int(df_meta[\"in_D_index\"].sum())\n",
    "n_strict = int(df_meta[\"in_D_strict\"].sum())\n",
    "\n",
    "print(f\"Rows in D time window: {n_time:,}, rows in index window: {n_index:,}, strict intersection: {n_strict:,}\")\n",
    "\n",
    "if n_strict == 0:\n",
    "    raise RuntimeError(\"No rows in strict Scenario D slice — check canonical indices/timestamps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca410016-0450-41a3-8056-8527902c84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: build a very gradual rising 7-day trend for Scenario D\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Window seconds\n",
    "window_sec = int((D_end_ts - D_start_ts).total_seconds())\n",
    "print(\"Scenario D window seconds:\", window_sec)\n",
    "\n",
    "# Very gradual rise: start ~0.28 -> end ~0.50 (tunable)\n",
    "start_val = 0.28\n",
    "end_val   = 0.50  # choose end so growth is visible but slow across 7 days\n",
    "\n",
    "# Linear trend then smoothed slightly for realism\n",
    "trend = np.linspace(start_val, end_val, window_sec)\n",
    "# gentle smoothing: sigma in seconds (e.g., 2 hours -> 7200)\n",
    "trend = gaussian_filter1d(trend, sigma=7200)\n",
    "trend = np.clip(trend, 0.0, 1.0)\n",
    "\n",
    "print(\"Trend sample (head/tail):\", trend[:5], \"...\", trend[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342f3c6-12a5-40d4-a242-7a1d305a2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: generate LF + HF noise and optionally a couple of tiny bursts (very rare)\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(20251123)\n",
    "\n",
    "# Noise parameters tuned down for very gradual drift\n",
    "lf_sigma_seconds = 7200      # long slow wiggles (~2 hours)\n",
    "lf_amp = 0.02                # low amplitude for LF\n",
    "hf_base = 0.008              # small high-frequency jitter\n",
    "row_jitter_sd = 0.003        # tiny per-row jitter\n",
    "\n",
    "# Low-frequency noise (filtered white noise)\n",
    "lf_white = rng.normal(0.0, 1.0, window_sec)\n",
    "lf_noise = gaussian_filter1d(lf_white, sigma=lf_sigma_seconds)\n",
    "lf_noise = lf_noise / (np.std(lf_noise) + 1e-12) * lf_amp\n",
    "\n",
    "# High-frequency noise\n",
    "hf_noise = rng.normal(0.0, hf_base, window_sec)\n",
    "\n",
    "combined_noise = lf_noise + hf_noise\n",
    "combined_noise = combined_noise - np.mean(combined_noise)  # zero-mean\n",
    "\n",
    "# Very few small bursts to add realism (0-2 across 7 days)\n",
    "num_bursts = rng.integers(0, 3)  # 0,1,2\n",
    "bursts = []\n",
    "for _ in range(num_bursts):\n",
    "    s = int(rng.integers(0, window_sec-60))\n",
    "    dur = int(rng.integers(20, 61))  # 20-60s\n",
    "    e = min(window_sec-1, s + dur)\n",
    "    amp = float(rng.uniform(0.03, 0.12))  # small additive amplitude\n",
    "    bursts.append((s, e, amp))\n",
    "    rel = np.linspace(0,1, e-s+1)\n",
    "    env = 1 - np.abs(2*rel - 1.0)  # triangular envelope\n",
    "    combined_noise[s:e+1] += env * amp\n",
    "\n",
    "print(\"LF std:\", float(np.std(lf_noise)), \"HF std:\", float(np.std(hf_noise)), \"num_bursts:\", num_bursts)\n",
    "print(\"Example bursts (if any):\", bursts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38d6b4-fc6f-4f77-91d5-74bd1628636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: compose final series and map to strict Scenario D rows (assign composite_score, health, labels)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Compose final series\n",
    "final_series = np.clip(trend + combined_noise, 0.0, 1.0)\n",
    "\n",
    "# Load full CSV (we need to update rows)\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "df[\"timestamp_parsed\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Build strict mask again (defensive)\n",
    "mask_index = df.index.to_series().between(D_start_idx0, D_end_idx0)\n",
    "mask_time = (df[\"timestamp_parsed\"] >= D_start_ts) & (df[\"timestamp_parsed\"] < D_end_ts)\n",
    "mask_strict = mask_time & mask_index\n",
    "\n",
    "n_to_update = int(mask_strict.sum())\n",
    "print(\"Rows to update (strict):\", n_to_update)\n",
    "\n",
    "if n_to_update == 0:\n",
    "    raise RuntimeError(\"No rows selected for Scenario D update. Aborting to avoid accidental edits.\")\n",
    "\n",
    "# Map timestamp -> seconds offset safely\n",
    "offsets = (df.loc[mask_strict, \"timestamp_parsed\"] - D_start_ts).dt.total_seconds().astype(int).clip(0, window_sec-1).values\n",
    "\n",
    "# Fetch values and add tiny per-row jitter\n",
    "vals = final_series[offsets] + rng.normal(0.0, row_jitter_sd, size=offsets.shape)\n",
    "vals = np.clip(vals, 0.0, 1.0)\n",
    "\n",
    "# Assign back only for strict mask\n",
    "df.loc[mask_strict, \"composite_score\"] = vals\n",
    "df.loc[mask_strict, \"composite_health\"] = 1.0 - df.loc[mask_strict, \"composite_score\"]\n",
    "\n",
    "# Recompute labels/severity for strict rows\n",
    "def label_and_sev(v):\n",
    "    if v > 0.6:\n",
    "        return \"anomaly\", 2\n",
    "    elif v > 0.4:\n",
    "        return \"suspicious\", 1\n",
    "    else:\n",
    "        return \"normal\", 0\n",
    "\n",
    "labs = [label_and_sev(v) for v in df.loc[mask_strict, \"composite_score\"].values]\n",
    "lab_col, sev_col = zip(*labs) if labs else ([], [])\n",
    "df.loc[mask_strict, \"anomaly_label\"] = lab_col\n",
    "df.loc[mask_strict, \"anomaly_severity\"] = sev_col\n",
    "\n",
    "print(\"Assigned composite_score + labels for Scenario D rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d2d2e-b1c6-427e-bd37-c0d50117687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: atomic write back to CSV (safe replace)\n",
    "import os\n",
    "\n",
    "tmp = CSV_PATH.with_suffix(\".tmp\")\n",
    "# drop helper column if present\n",
    "if \"timestamp_parsed\" in df.columns:\n",
    "    df_to_write = df.drop(columns=[\"timestamp_parsed\"])\n",
    "else:\n",
    "    df_to_write = df\n",
    "df_to_write.to_csv(tmp, index=False)\n",
    "os.replace(tmp, CSV_PATH)\n",
    "print(\"CSV updated in place:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef11198-d71d-4b63-ba0e-64736a54dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Diagnostics + compressed timeline plot for A (first 12d) + B + C + D (showing D gradual rise)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# reload minimal columns\n",
    "use_cols = [\"timestamp\", \"composite_score\"]\n",
    "df_plot = pd.read_csv(CSV_PATH, usecols=use_cols)\n",
    "df_plot = df_plot.dropna(subset=[\"timestamp\", \"composite_score\"]).reset_index(drop=True)\n",
    "df_plot[\"timestamp\"] = pd.to_datetime(df_plot[\"timestamp\"], utc=True)\n",
    "df_plot = df_plot.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# define combined window (A start .. D end)\n",
    "start_date = df_plot[\"timestamp\"].min().normalize()\n",
    "A_first12_end = start_date + pd.Timedelta(days=12)\n",
    "# B_start/B_end infer (safe)\n",
    "B_start = df_plot[df_plot[\"timestamp\"] >= A_first12_end][\"timestamp\"].min()\n",
    "B_end = B_start + pd.Timedelta(days=7)\n",
    "# C_start we derive from canonical index 320400 (0-based)\n",
    "C_start = df_plot.loc[320400, \"timestamp\"]\n",
    "C_end = C_start + pd.Timedelta(days=7)\n",
    "# D_start from canonical index (we computed earlier)\n",
    "D_start = df_plot.loc[D_start_idx0, \"timestamp\"]\n",
    "D_end = D_start + pd.Timedelta(days=7)\n",
    "\n",
    "print(\"Window dates: A start:\", start_date, \"-> D end:\", D_end)\n",
    "\n",
    "# Select plotting window: from A start to D end (inclusive)\n",
    "mask_window = (df_plot[\"timestamp\"] >= start_date) & (df_plot[\"timestamp\"] < D_end)\n",
    "df_sel = df_plot.loc[mask_window].copy().reset_index(drop=True)\n",
    "print(\"Rows plotted:\", len(df_sel))\n",
    "\n",
    "# compress timeline by removing gaps > 30 minutes\n",
    "GAP_MINUTES = 30\n",
    "gap_thresh = pd.Timedelta(minutes=GAP_MINUTES)\n",
    "ts = df_sel[\"timestamp\"]\n",
    "diffs = ts.diff().fillna(pd.Timedelta(seconds=0))\n",
    "is_segment_start = (diffs > gap_thresh)\n",
    "segment_id = is_segment_start.cumsum()\n",
    "\n",
    "prev_end = ts.iloc[0]\n",
    "shifts = pd.Series(pd.Timedelta(0), index=df_sel.index)\n",
    "cumulative_shift = pd.Timedelta(0)\n",
    "for seg_idx, (_, grp) in enumerate(df_sel.groupby(segment_id)):\n",
    "    seg_start = grp[\"timestamp\"].iloc[0]\n",
    "    seg_end = grp[\"timestamp\"].iloc[-1]\n",
    "    if seg_idx == 0:\n",
    "        cumulative_shift = pd.Timedelta(0)\n",
    "    else:\n",
    "        gap = seg_start - prev_end\n",
    "        cumulative_shift += gap\n",
    "    shifts.loc[grp.index] = cumulative_shift\n",
    "    prev_end = seg_end\n",
    "\n",
    "df_sel[\"shifted_ts\"] = df_sel[\"timestamp\"] - shifts.values\n",
    "\n",
    "# compute 1-minute resample and 1-hour rolling mean\n",
    "df_shift = df_sel.set_index(\"shifted_ts\").sort_index()\n",
    "resampled = df_shift[\"composite_score\"].resample(\"0.1T\").mean().interpolate(limit_direction='both')\n",
    "hr_line = resampled.rolling(window=60, min_periods=1).mean()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.scatter(df_shift.index, df_shift[\"composite_score\"], s=5, alpha=0.20, label=\"composite_score (points)\")\n",
    "plt.plot(hr_line.index, hr_line.values, linewidth=2.2, color=\"#D62728\", label=\"1-hour rolling mean\")\n",
    "\n",
    "# compute shifted spans for B, C, D for shading\n",
    "def shifted_bounds(win_start, win_end, df_sel):\n",
    "    rows = df_sel[(df_sel[\"timestamp\"] >= win_start) & (df_sel[\"timestamp\"] < win_end)]\n",
    "    if rows.empty:\n",
    "        return None, None\n",
    "    return rows[\"shifted_ts\"].iloc[0], rows[\"shifted_ts\"].iloc[-1]\n",
    "\n",
    "b_s, b_e = shifted_bounds(B_start, B_end, df_sel)\n",
    "c_s, c_e = shifted_bounds(C_start, C_end, df_sel)\n",
    "d_s, d_e = shifted_bounds(D_start, D_end, df_sel)\n",
    "\n",
    "if b_s is not None:\n",
    "    plt.axvspan(b_s, b_e, color=\"gray\", alpha=0.12, label=\"Scenario B\")\n",
    "if c_s is not None:\n",
    "    plt.axvspan(c_s, c_e, color=\"orange\", alpha=0.12, label=\"Scenario C\")\n",
    "if d_s is not None:\n",
    "    plt.axvspan(d_s, d_e, color=\"green\", alpha=0.12, label=\"Scenario D (gradual)\")\n",
    "\n",
    "plt.title(\"Composite Score — A + B + C + D (compressed timeline). Scenario D shows gradual upward trend.\")\n",
    "plt.xlabel(\"Compressed operating time (gaps removed)\")\n",
    "plt.ylabel(\"Composite Score\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnostics for D-window\n",
    "df_d = df_sel[(df_sel[\"timestamp\"] >= D_start) & (df_sel[\"timestamp\"] < D_end)].copy()\n",
    "if not df_d.empty:\n",
    "    df_d[\"date_only\"] = df_d[\"timestamp\"].dt.date\n",
    "    perday = df_d.groupby(\"date_only\")[\"composite_score\"].agg(['count','mean','std','min','max']).reset_index()\n",
    "    print(\"\\nPer-day composite summary for Scenario D:\")\n",
    "    print(perday.to_string(index=False))\n",
    "    # Detect anomaly segments (threshold 0.6)\n",
    "    thr = 0.6\n",
    "    df_d[\"is_anom\"] = (df_d[\"composite_score\"] > thr).astype(int)\n",
    "    df_d[\"seg_id\"] = (df_d[\"is_anom\"] != df_d[\"is_anom\"].shift(1)).cumsum()\n",
    "    segs = []\n",
    "    for seg_id, grp in df_d.groupby(\"seg_id\"):\n",
    "        if grp[\"is_anom\"].iloc[0] == 1:\n",
    "            start = grp[\"timestamp\"].iloc[0]; end = grp[\"timestamp\"].iloc[-1]\n",
    "            segs.append({\"seg_id\":int(seg_id), \"start\":str(start), \"end\":str(end), \"dur_s\":int((end-start).total_seconds()), \"n\":len(grp)})\n",
    "    segs_sorted = sorted(segs, key=lambda x: x[\"dur_s\"], reverse=True)\n",
    "    print(f\"\\nDetected {len(segs_sorted)} anomaly segments in D-window (threshold>{thr}). Top segments:\")\n",
    "    for s in segs_sorted[:10]:\n",
    "        print(f\"  seg {s['seg_id']}: start={s['start']}, dur_s={s['dur_s']}, n={s['n']}\")\n",
    "else:\n",
    "    print(\"No rows found in D-window for diagnostics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6e157-6da4-4fc1-a69f-e1f6439a8f86",
   "metadata": {},
   "source": [
    "### Scenario E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d3243-e0de-43fb-9046-22e9f6e2d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Load CSV + Compute Scenario E window (strict index + timestamp)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ishaa\\OneDrive\\Desktop\\synthetic_data_final\\synthetic_battery_inference_scenarioA.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(CSV_PATH)\n",
    "\n",
    "# Load timestamps only for speed\n",
    "df_meta = pd.read_csv(CSV_PATH, usecols=[\"timestamp\"])\n",
    "df_meta[\"timestamp_parsed\"] = pd.to_datetime(df_meta[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "total_rows = len(df_meta)\n",
    "print(\"Total rows:\", total_rows)\n",
    "\n",
    "# Scenario E start (1-based) -> convert to 0-based\n",
    "E_start_1 = 604801\n",
    "E_start_idx0 = E_start_1 - 1\n",
    "\n",
    "if E_start_idx0 >= total_rows:\n",
    "    raise RuntimeError(\"Start index exceeds file length.\")\n",
    "\n",
    "# Extract Scenario E start timestamp\n",
    "E_start_ts = df_meta.loc[E_start_idx0, \"timestamp_parsed\"]\n",
    "E_end_ts = E_start_ts + pd.Timedelta(days=7)\n",
    "\n",
    "print(\"Scenario E start index:\", E_start_1)\n",
    "print(\"Scenario E start timestamp:\", E_start_ts)\n",
    "print(\"Scenario E end timestamp (7 days after):\", E_end_ts)\n",
    "\n",
    "# Build masks\n",
    "mask_index = df_meta.index.to_series().between(E_start_idx0, total_rows - 1)\n",
    "mask_time = (df_meta[\"timestamp_parsed\"] >= E_start_ts) & (df_meta[\"timestamp_parsed\"] < E_end_ts)\n",
    "mask_E_strict = mask_index & mask_time\n",
    "\n",
    "n_time = int(mask_time.sum())\n",
    "n_index = int(mask_index.sum())\n",
    "n_strict = int(mask_E_strict.sum())\n",
    "\n",
    "print(f\"Rows in time window: {n_time:,}\")\n",
    "print(f\"Rows in index window: {n_index:,}\")\n",
    "print(f\"Rows in strict Scenario-E slice:\", n_strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93dff6a-c749-47c0-9e13-7f480f144e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Build 7-day healthy flat trend for Scenario E\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Compute exact duration in seconds (from timestamps)\n",
    "E_window_secs = int((E_end_ts - E_start_ts).total_seconds())\n",
    "print(\"Scenario E window seconds:\", E_window_secs)\n",
    "\n",
    "# Flat healthy baseline: 0.18 → 0.22 over 7 days\n",
    "base_start = 0.18\n",
    "base_end   = 0.22\n",
    "\n",
    "trend_E = np.linspace(base_start, base_end, E_window_secs)\n",
    "\n",
    "# Gentle smoothing (slightly smoother than scenario D)\n",
    "trend_E = gaussian_filter1d(trend_E, sigma=5400)  # ~1.5 hours smoothing\n",
    "trend_E = np.clip(trend_E, 0, 1)\n",
    "\n",
    "print(\"Trend sample:\", trend_E[:5], \"...\", trend_E[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee8445-190b-4c48-b188-607b3a2a54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — LF noise + HF jitter + rare micro-blips for Scenario E\n",
    "\n",
    "rng = np.random.default_rng(20251123)\n",
    "\n",
    "# Very gentle noise for healthy scenario\n",
    "lf_sigma = 8800         # long-term drift ~2.5 hours\n",
    "lf_amp   = 0.015        # small amplitude slow wiggle\n",
    "\n",
    "hf_sd = 0.007           # small HF jitter\n",
    "row_sd = 0.003          # minor per-row jitter\n",
    "\n",
    "# Low-frequency noise\n",
    "lf_white = rng.normal(0, 1, E_window_secs)\n",
    "lf_noise = gaussian_filter1d(lf_white, sigma=lf_sigma)\n",
    "lf_noise = lf_noise / (np.std(lf_noise) + 1e-9) * lf_amp\n",
    "\n",
    "# High-frequency jitter\n",
    "hf_noise = rng.normal(0, hf_sd, E_window_secs)\n",
    "\n",
    "# Combine and zero-mean LF+HF\n",
    "combined_noise_E = lf_noise + hf_noise\n",
    "combined_noise_E -= combined_noise_E.mean()\n",
    "\n",
    "# Micro-blips: 1–3 tiny spikes in 7 days\n",
    "num_blips = rng.integers(1, 4)\n",
    "blips = []\n",
    "for _ in range(num_blips):\n",
    "    s = int(rng.integers(0, E_window_secs - 60))\n",
    "    d = int(rng.integers(10, 40))\n",
    "    e = min(E_window_secs - 1, s + d)\n",
    "    amp = float(rng.uniform(0.04, 0.12))\n",
    "    blips.append((s, e, amp))\n",
    "\n",
    "    rel = np.linspace(0, 1, e - s + 1)\n",
    "    tri = 1 - np.abs(2 * rel - 1)\n",
    "    combined_noise_E[s:e+1] += tri * amp\n",
    "\n",
    "print(\"LF std:\", np.std(lf_noise))\n",
    "print(\"HF std:\", np.std(hf_noise))\n",
    "print(\"Micro-blips:\", blips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94507d6-05a6-4e70-9666-02aa4f349edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Compose final series and update CSV rows for Scenario E\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "final_E = np.clip(trend_E + combined_noise_E, 0, 1)\n",
    "\n",
    "# Load full CSV to update\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "df[\"timestamp_parsed\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Recompute strict mask\n",
    "mask_index = df.index.to_series().between(E_start_idx0, total_rows - 1)\n",
    "mask_time = (df[\"timestamp_parsed\"] >= E_start_ts) & (df[\"timestamp_parsed\"] < E_end_ts)\n",
    "mask_E_strict = mask_index & mask_time\n",
    "\n",
    "n_update = int(mask_E_strict.sum())\n",
    "print(\"Updating Scenario E rows:\", n_update)\n",
    "\n",
    "if n_update == 0:\n",
    "    raise RuntimeError(\"No Scenario E rows found!\")\n",
    "\n",
    "# Offsets in seconds\n",
    "offsets = (df.loc[mask_E_strict, \"timestamp_parsed\"] - E_start_ts).dt.total_seconds().astype(int)\n",
    "offsets = offsets.clip(0, E_window_secs - 1).values\n",
    "\n",
    "vals = final_E[offsets] + rng.normal(0, row_sd, size=len(offsets))\n",
    "vals = np.clip(vals, 0, 1)\n",
    "\n",
    "df.loc[mask_E_strict, \"composite_score\"] = vals\n",
    "df.loc[mask_E_strict, \"composite_health\"] = 1 - vals\n",
    "\n",
    "# Label logic\n",
    "def classify(v):\n",
    "    if v > 0.6:\n",
    "        return \"anomaly\", 2\n",
    "    elif v > 0.4:\n",
    "        return \"suspicious\", 1\n",
    "    return \"normal\", 0\n",
    "\n",
    "labels = [classify(v) for v in vals]\n",
    "lab, sev = zip(*labels)\n",
    "\n",
    "df.loc[mask_E_strict, \"anomaly_label\"] = lab\n",
    "df.loc[mask_E_strict, \"anomaly_severity\"] = sev\n",
    "\n",
    "print(\"Scenario E composite_score/labels updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837e66d-f28a-4f31-9df0-fcd1d3faaf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Safe atomic write\n",
    "\n",
    "import os\n",
    "\n",
    "tmp = CSV_PATH.with_suffix(\".tmp\")\n",
    "df.drop(columns=[\"timestamp_parsed\"]).to_csv(tmp, index=False)\n",
    "os.replace(tmp, CSV_PATH)\n",
    "\n",
    "print(\"CSV updated in place:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24ea0c-5c41-45dd-8127-9df79b44fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Compressed timeline plot including Scenario E\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dfp = pd.read_csv(CSV_PATH, usecols=[\"timestamp\", \"composite_score\"])\n",
    "dfp[\"timestamp\"] = pd.to_datetime(dfp[\"timestamp\"], utc=True)\n",
    "dfp = dfp.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "start_ts = dfp[\"timestamp\"].min()\n",
    "end_ts   = E_end_ts\n",
    "\n",
    "mask = (dfp[\"timestamp\"] >= start_ts) & (dfp[\"timestamp\"] < end_ts)\n",
    "dfp = dfp.loc[mask].copy()\n",
    "\n",
    "ts = dfp[\"timestamp\"]\n",
    "diffs = ts.diff().fillna(pd.Timedelta(seconds=0))\n",
    "seg_start = diffs > pd.Timedelta(minutes=30)\n",
    "seg_id = seg_start.cumsum()\n",
    "\n",
    "shifts = pd.Series(pd.Timedelta(0), index=dfp.index)\n",
    "prev_end = ts.iloc[0]\n",
    "cum_shift = pd.Timedelta(0)\n",
    "\n",
    "for i, (_, g) in enumerate(dfp.groupby(seg_id)):\n",
    "    s = g[\"timestamp\"].iloc[0]\n",
    "    e = g[\"timestamp\"].iloc[-1]\n",
    "    if i > 0:\n",
    "        gap = s - prev_end\n",
    "        cum_shift += gap\n",
    "    shifts.loc[g.index] = cum_shift\n",
    "    prev_end = e\n",
    "\n",
    "dfp[\"shifted_ts\"] = dfp[\"timestamp\"] - shifts.values\n",
    "dfp = dfp.set_index(\"shifted_ts\")\n",
    "\n",
    "res = dfp[\"composite_score\"].resample(\"0.31T\").mean().interpolate()\n",
    "hr = res.rolling(60, min_periods=1).mean()\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.scatter(dfp.index, dfp[\"composite_score\"], s=4, alpha=0.18)\n",
    "plt.plot(hr.index, hr.values, linewidth=2.0, color=\"orange\")\n",
    "\n",
    "plt.title(\"Composite Score — Scenarios A + B + C + D + E (Compressed Timeline)\")\n",
    "plt.xlabel(\"Compressed time\")\n",
    "plt.ylabel(\"Composite Score\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2c2fb-ce52-4316-b112-fbfb80226b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
